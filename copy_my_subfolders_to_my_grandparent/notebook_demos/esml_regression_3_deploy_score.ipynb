{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Init ESML "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\r\n",
    "from esml import ESMLProject\r\n",
    "\r\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\r\n",
    "p.active_model = 11\r\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\r\n",
    "p.inference_mode = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unregister_all_datasets=False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if(unregister_all_datasets):\r\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose\r\n",
    "\r\n",
    "p.init()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Test an `existing` Model webservice (AKS)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2a) Lets simulate INFERENCE: data arives from CLIENT to a `DriverAPI` that itself will call `this ModelAPI` (AKS)\n",
    "Note: `Step 1 and 2 are out of scope` of from the MACHINE LEARNING solution (and ESML), other than...\n",
    " - `ESML` being the creator of AML pipeline - `Bronze_2_Gold` that you can reuse\n",
    " - `ESML lakedesign`, that should be used, if INFERENCE are to be stored/cached in the lake\n",
    "\n",
    "NOW - the scenario: \n",
    "\n",
    "- DriverAPI will:\n",
    "\n",
    "    - 1)Fetch missing datasources from different systems, to get all data sources ds01, ds02,ds03 & SAVE to INFERENCE/IN folder in the datalake\n",
    "        - Use the `ESML` lakedesign:  IF `inference_model_version=1` in `settings/project_specific/lake_settings.json` then...\n",
    "        - `ESML` will switch to write at `INFERENCE/v1/ds01/IN/2020/01/01` folder \n",
    "    - 2)Call `Bronze_2_Gold` batch pipeline (AML) - same pipeline that was used for training: `IN->Bronze->Silver->Gold`, except...\n",
    "        - `Bronze_2_Gold` pipeline will read from IN folder, but configured for \"INFERENCE\", read/write to `\"mirror\"` place in the datalake, that will `CACHE` online predicted results\n",
    "        - (LABEL values are missing of course)\n",
    "    - 3) `DriverAPI calls ModelAPI` (This AKS webservuce) with data to score -> `X_text`\n",
    "\n",
    "- `ESML` ModelAPI will:\n",
    "    - 4) `Score` the data, and return results, also `save the results to the datalake`\n",
    "        - if `p.rnd=True` nothing is stored.\n",
    "    - 5) `Connect the scored data` to a caller, and the individual scores to a identity (user/machine)\n",
    "        - GLOBALLY & ESML Managed: The scoring file gets a caller_id. \n",
    "            - You: Need to pass the called-guid as a parameter. \n",
    "                - Alternatively, you can have a column in the dataframe called `esml_caller_id_string`, and ESML will use that.\n",
    "        - LOCALLY: The rows in the dataframe has an identity. \n",
    "            - You: Here you need to have a column in the dataframe, such as `user_id`, to be able to connect each row to a scoring.\n",
    "\n",
    "\n",
    "Step 1 and 2 are out of scope of from the MACHINE LEARNING solution  \n",
    "We START this notebooke at step 3 - we have `X_test` in `GOLD_Validate` status "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime as dt\r\n",
    "now = dt.datetime.now()\r\n",
    "folder = now.strftime('%Y_%m_%d') \r\n",
    "print(folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if(unregister_all_datasets): # Create a GOLD dataset, and SPLIT it\r\n",
    "    df_01 = p.DatasetByName(\"ds01_diabetes\").Silver.to_pandas_dataframe()\r\n",
    "    ds_gold_v1 = p.save_gold(df_01)\r\n",
    "\r\n",
    "    label = p.active_model[\"label\"]\r\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6, label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.GoldValidate.name"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2) Bronze_2_Gold done, we can fetch X_test\r\n",
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest. \r\n",
    "print(tags)\r\n",
    "#Note: If you get error: AttributeError: 'NoneType' object has no attribute 'name'...you need to have splitted sot you have a GoldValidate Dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b) Call onlin ModelAPI - score 1 row (alt 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "import pandas as pd\n",
    "#3) `DriverAPI calls ModelAPI`\n",
    "\n",
    "keyvault = p.ws.get_default_keyvault()\n",
    "api_url = keyvault.get_secret(name=\"esml-dev-p02-m03-api\")\n",
    "api_key = keyvault.get_secret(name=\"esml-dev-p02-m03-apisecret\")  # esml-dev-p02-m03-apisecret\n",
    "\n",
    "df = ESMLProject.call_webservice_own_url(X_test, api_url,api_key) # rest call. STATIC method - simulate \"we dont need ESML for this\" - just an URL and KEY from DriverAPI\n",
    "df.head() # Print scoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b) Call online ModelAPI - score all rows (alt 2) - `via ESML wrapper`\n",
    "- ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "- Note: We can also save a GUID/ID, `to see which CALLER/User-Guid the scoring is about` \n",
    "    - We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter in the call"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p.inferenceModelVersion=1\r\n",
    "print(p.ScoredPath)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "caller_user_id = '81965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\r\n",
    "\r\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) How to GET SCORE data? how to FILTER scored results?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy()\r\n",
    "caller_user_id = '91965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\r\n",
    "\r\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime as dt\r\n",
    "now = dt.datetime.now()\r\n",
    "date_filter = now.strftime('%Y_%m_%d') \r\n",
    "print(date_filter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"10\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#df = p.get_scored('last_month')\r\n",
    "ds_list1, df_all1 = p.get_scored(date_filter, \"1\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')\r\n",
    "print(\"Datasets found in filter\", len(ds_list1))\r\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list1[0].to_pandas_dataframe().shape[0])\r\n",
    "print(\"All rows\", df_all1.shape[0])\r\n",
    "\r\n",
    "df_all1.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"1\")\r\n",
    "print(\"Datasets found in filter\", len(ds_list))\r\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list[0].to_pandas_dataframe().shape[0])\r\n",
    "print(\"All rows\", df_all.shape[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " ** Train model & register ....this happended already in another notebook/MLops pipline step\n",
    "> model = p.register_active_model()  \n",
    "> print(model.name, model.description, model.version)  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3a) ESML `Deploy a new ONLINE` webservice (AKS)\n",
    "- Deploy \"offline\" from old `AutoML run` for `DEV` environment\n",
    "- To →  `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(p.ws) #  AutoML support \r\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config, True) # overwrite_endpoint=True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the NEW AKS WebService"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest\r\n",
    "print(tags)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = p.call_webservice(p.ws, X_test) # Auto-fetch key from keyvault, 1stRowOnlye=False  |   p.call_webservice(p.ws, X_test, caller_user_id, False)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code to `embed in YOUR DriverApi` - to call this webservice `without ESML`\n",
    "- `NB!` This will `NOT cache` the results automatically, since not going via the ESML SDK. You need to save the scoring yourself ( if this is needed)\n",
    "    - ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "            # Note: We can also save a GUID/ID, for which CALLER/User-Guid it is about. We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def call_webservice_code(rows, api_uri,api_key, allowSelfSigned=True):\r\n",
    "    # bypass the server certificate verification on client side\r\n",
    "    if allowSelfSigned and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\r\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\r\n",
    "\r\n",
    "    X_test_json_works = json.dumps({'data': rows.to_dict(orient='records')})\r\n",
    "\r\n",
    "    headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + api_key}\r\n",
    "    resp = requests.post(api_uri, X_test_json_works , headers=headers) \r\n",
    "\r\n",
    "    # Here you can pass forward the response, save it to the datalake, or as below return a PANDAS dataframe\r\n",
    "    res_dict = json.loads(resp.text)\r\n",
    "    res_dict_ast = ast.literal_eval(res_dict)\r\n",
    "    return pd.read_json(res_dict) # to pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3b) ESML `Deploy BATCH` pipeline\n",
    "- Deploy same model \"offline / previous\" `AutoML Run` for `DEV` environment\n",
    "- To →  `DEV`, `TEST` or `PROD` environment\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5a alt2) If not using AutoML - you need to manuallt create `environemnt + entryscript + inference config`\n",
    "https://github.com/Azure/MachineLearningNotebooks/blob/bda592a236eaf2dbc54b394e1fa1b539e0297908/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create ENVIRONMENT - If not AUTOML\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies \r\n",
    "\r\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=['azureml-defaults', 'inference-schema'])\r\n",
    "myenv = Environment(name='myenv')\r\n",
    "myenv.python.conda_dependencies = conda_deps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile score.py\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import json\r\n",
    "import numpy\r\n",
    "from sklearn.externals import joblib\r\n",
    "from sklearn.linear_model import Ridge\r\n",
    "\r\n",
    "def init():\r\n",
    "    global model\r\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\r\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\r\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\r\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\r\n",
    "    # deserialize the model file back into a sklearn model\r\n",
    "    model = joblib.load(model_path)\r\n",
    "\r\n",
    "# note you can pass in multiple rows for scoring\r\n",
    "def run(raw_data):\r\n",
    "    try:\r\n",
    "        data = json.loads(raw_data)['data']\r\n",
    "        data = numpy.array(data)\r\n",
    "        result = model.predict(data)\r\n",
    "        # you can return any data type as long as it is JSON-serializable\r\n",
    "        return result.tolist()\r\n",
    "    except Exception as e:\r\n",
    "        error = str(e)\r\n",
    "        return error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.model import InferenceConfig\r\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OTHER - What more can you retrieve from AutoMLFactory and ESMLProject?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target_exp, target_model,target_run, target_best_run,fitted_model = p.get_best_model_and_run_via_experiment_name_and_ws(p.ws)\r\n",
    "target_best_run_id = target_model.tags[\"run_id\"] # model.tags.get(\"run_id\") Example: AutoML_08bb87d4-9587-4b99-b781-fe16bd13f140\r\n",
    "target_model_name = target_model.tags[\"model_name\"]\r\n",
    "target_best_model_version = target_model.version\r\n",
    "\r\n",
    "print(\"Target found (registered):\")\r\n",
    "print(\" Target - best_run_id\", target_best_run_id)\r\n",
    "print(\" Target - best_run_id (best run)\", target_best_run.id)\r\n",
    "print(\" Target - model_name (from model.tag)\",target_model_name)\r\n",
    "print(\" Target - model_version\",target_best_model_version)\r\n",
    "\r\n",
    "run, exp = p.get_active_model_run_and_experiment()\r\n",
    "inference_config = p.get_active_model_inference_config()\r\n",
    "print(run)\r\n",
    "print(exp.name)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}