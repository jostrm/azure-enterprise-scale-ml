{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)"
  },
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1) Init ESML "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inference version: 1\n\n - ds01_diabetes\nprojects/project002/03_diabetes_model_reg/inference/1/ds01_diabetes/in/dev/2021/06/08/\nprojects/project002/03_diabetes_model_reg/inference/1/ds01_diabetes/out/bronze/dev/\nprojects/project002/03_diabetes_model_reg/inference/1/ds01_diabetes/out/silver/dev/\n\n - ds02_other\nprojects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\nprojects/project002/03_diabetes_model_reg/inference/1/ds02_other/out/bronze/dev/\nprojects/project002/03_diabetes_model_reg/inference/1/ds02_other/out/silver/dev/\n \n\nTraining GOLD (p.GoldPath)\nprojects/project002/03_diabetes_model_reg/inference/1/gold/dev/\n \n\n[A) USAGE]: to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\nA)INFERENCE ONLINE: GOLD to score (example if realtime - today)\nprojects/project002/03_diabetes_model_reg/inference/1/gold/dev/2021_06_21/921ebc22894c4426a04bfd08bb0be450/\n \n\nA)INFERENCE ONLINE: GOLD scored (example if realtime today)\nprojects/project002/03_diabetes_model_reg/inference/1/scored/dev/2021_06_21/921ebc22894c4426a04bfd08bb0be450/\n \n\n[B) USAGE]: to_score_folder_batch, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)\nB)INFERENCE BATCH: GOLD to score (example batch, datetime from config)\nprojects/project002/03_diabetes_model_reg/inference/1/gold/dev/2021_06_08/d5186fa2794b4245b57c2778fddcd6f4/\n \n\nB)INFERENCE BATCH: GOLD scored (example batch, datetime from config)\nprojects/project002/03_diabetes_model_reg/inference/1/scored/dev/2021_06_08/d5186fa2794b4245b57c2778fddcd6f4/\n \n\nC) INFERENCE BATCH (SCENARIO 2): TODAY I scored data from X days AGO  (second datefolder from config - X days ago)\nprojects/project002/03_diabetes_model_reg/inference/1/gold/dev/2021_06_08/d5186fa2794b4245b57c2778fddcd6f4/2021_06_08/\nprojects/project002/03_diabetes_model_reg/inference/1/scored/dev/2021_06_08/d5186fa2794b4245b57c2778fddcd6f4/2021_06_08/\n \n\nENVIRONMENT - DEV, TEST, or PROD?  [USAGE: p.dev_test_prod]\nACTIVE ENVIRONMENT = dev\nACTIVE subscription = ca0a8c40-b06a-4e4e-8434-63c03a1dee34\n- MSFT-WEU-EAP_PROJECT02_AI-DEV-RG\n- msft-weu-DEV-eap-proj02_ai-amls\n- westeurope\n- MSFT-WEU-EAP_CMN_AI-DEV-RG\nActive vNet: msft-weu-dev-cmnai-vnet\nActive SubNet: \n[USAGE] for the above: p.vNetForActiveEnvironment()\nActive Lake (storage account)  msftweudevcmnai2\n[USAGE] for the above: p.getLakeForActiveEnvironment()\nAML for docker: True\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "from azureml.core import Workspace\n",
    "\n",
    "p = ESMLProject() # Makes it \"environment aware (dev,test,prod)\", and \"configuration aware\"\n",
    "ws = p.get_workspace_from_config()\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unregister_all_datasets=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "...\n",
      "Using GEN2 as Datastore\n",
      "Searching for setting in ESML datalake...\n",
      "ESML in-folder settings override = TRUE \n",
      " - Found settings in the ESML AutoLake  [active_in_folder.json,active_scoring_in_folder.json], to override ArgParse/GIT config with.\n",
      " - TRAIN in date:  2021/01/01\n",
      " - INFERENCE in date: 2021/06/08 and ModelVersion to score with: 1 (0=latest)\n",
      "\n",
      "Inference mode (False = Training mode): True\n",
      "Load data as Datasets....\n",
      "ds01_diabetes\n",
      "ds02_other\n",
      "IN (.csv or .parquet) coult not be initiated  for dataset ds02_other with description IN:  ds02_other. Trying as .parquet instead.\n",
      "ESML Note: Could not read InData (Scoring) as .CSV - Now trying as .PARQUET instead.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "UserErrorException",
     "evalue": "UserErrorException:\n\tMessage: Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team. \n Tip_1: Since INFERENCE MODE=TRUE, check that your Ingestion-pipeline (Azure Data factory) to IN-folder is successful. Check in lake if: correct model_version=1 and correct date_folder\n Tip_2: Maybe data is deleted in lake? But dataset is still registered? Check here in the datalake with Storage Explorer: projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team. \\n Tip_1: Since INFERENCE MODE=TRUE, check that your Ingestion-pipeline (Azure Data factory) to IN-folder is successful. Check in lake if: correct model_version=1 and correct date_folder\\n Tip_2: Maybe data is deleted in lake? But dataset is still registered? Check here in the datalake with Storage Explorer: projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\"\n    }\n}",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_validate_has_data\u001b[1;34m(dataflow, error_message)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mdataflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify_has_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     except (dataprep().api.dataflow.DataflowValidationError,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py\u001b[0m in \u001b[0;36mverify_has_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_pyrecords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mDataflowValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The Dataflow produced no records.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py\u001b[0m in \u001b[0;36m_to_pyrecords\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    794\u001b[0m                         \u001b[0manonymous_activity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataflow_to_anonymous_activity_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataflow_to_execute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                         \u001b[0mspan_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_dprep_span_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m                     ))\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\api.py\u001b[0m in \u001b[0;36mexecute_anonymous_activity\u001b[1;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_anonymous_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecuteAnonymousActivityMessageArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Engine.ExecuteActivity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\engine.py\u001b[0m in \u001b[0;36msend_message\u001b[1;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mcancel_on_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mraise_engine_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\errorhandlers.py\u001b[0m in \u001b[0;36mraise_engine_error\u001b[1;34m(error_response)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'ScriptExecution'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mExecutionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'Validation'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: 45725c2a-dcdb-4fe2-aa17-27170b0ac788\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://msftweudevcmnai2.dfs.core.windows.net/lake3?directory=projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID 'fc9db4be-e1ca-4971-adbe-ace01675fa65', request ID '3bf2f519-f01f-000c-279c-66f7d8000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:3bf2f519-f01f-000c-279c-66f7d8000000\\nTime:2021-06-21T12:50:47.0706895Z\"}}\r\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDatasetValidationError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mautomap_and_register_aml_datasets\u001b[1;34m(self, ws, dataset_name, dataset_description_in)\u001b[0m\n\u001b[0;32m   1612\u001b[0m                     \u001b[0merror_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m                     \u001b[0min_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTabular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_delimited_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdstore_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create the Tabular dataset with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1614\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_factory.py\u001b[0m in \u001b[0;36mfrom_delimited_files\u001b[1;34m(path, validate, include_path, infer_column_types, set_column_types, separator, header, partition_format, support_multi_line, empty_as_string, encoding)\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[0mdataflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m             validate or infer_column_types or _is_inference_required(set_column_types))\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfer_column_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_factory.py\u001b[0m in \u001b[0;36m_transform_and_validate\u001b[1;34m(dataflow, partition_format, include_path, validate)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         _validate_has_data(dataflow, 'Cannot load any data from the specified path. '\n\u001b[0m\u001b[0;32m   1063\u001b[0m                                      'Make sure the path is accessible and contains data.')\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_validate_has_data\u001b[1;34m(dataflow, error_message)\u001b[0m\n\u001b[0;32m     67\u001b[0m             dataprep().api.errorhandlers.ExecutionError) as e:\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mDatasetValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompliant_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDatasetValidationError\u001b[0m: DatasetValidationError:\n\tMessage: Cannot load any data from the specified path. Make sure the path is accessible and contains data.\nScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID 'fc9db4be-e1ca-4971-adbe-ace01675fa65', request ID '3bf2f519-f01f-000c-279c-66f7d8000000'. Error message: [REDACTED]\r\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Cannot load any data from the specified path. Make sure the path is accessible and contains data.\\nScriptExecutionException was caused by StreamAccessException.\\r\\n  StreamAccessException was caused by NotFoundException.\\r\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID 'fc9db4be-e1ca-4971-adbe-ace01675fa65', request ID '3bf2f519-f01f-000c-279c-66f7d8000000'. Error message: [REDACTED]\\r\\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de\"\n    }\n}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_try_execute\u001b[1;34m(action, operation, dataset_info, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\tabular_dataset.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m         df = _try_execute(lambda: dataflow.to_pandas_dataframe(on_error=on_error,\n\u001b[1;32m--> 169\u001b[1;33m                                                                out_of_range_datetime=out_of_range_datetime),\n\u001b[0m\u001b[0;32m    170\u001b[0m                           \u001b[1;34m'to_pandas_dataframe'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py\u001b[0m in \u001b[0;36mto_pandas_dataframe\u001b[1;34m(self, extended_types, nulls_as_nan, on_error, out_of_range_datetime)\u001b[0m\n\u001b[0;32m    700\u001b[0m                                                               \u001b[0mout_of_range_datetime\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m                                                               to_dprep_span_context(span.get_context()))\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py\u001b[0m in \u001b[0;36mto_pandas_dataframe\u001b[1;34m(self, dataflow, extended_types, nulls_as_nan, on_error, out_of_range_datetime, span_context)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mto_pandas_feather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py\u001b[0m in \u001b[0;36mto_pandas_feather\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m             dataflow._engine_api.execute_anonymous_activity(\n\u001b[1;32m--> 232\u001b[1;33m                 ExecuteAnonymousActivityMessageArguments(anonymous_activity=activity_data, span_context=span_context))\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\_aml_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mengine_api_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_environment_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchanged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msend_message_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\api.py\u001b[0m in \u001b[0;36mexecute_anonymous_activity\u001b[1;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_anonymous_activity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtypedefinitions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExecuteAnonymousActivityMessageArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mCancellationToken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_channel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Engine.ExecuteActivity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\engine.py\u001b[0m in \u001b[0;36msend_message\u001b[1;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mcancel_on_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mraise_engine_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\dataprep\\api\\errorhandlers.py\u001b[0m in \u001b[0;36mraise_engine_error\u001b[1;34m(error_response)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'ScriptExecution'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mExecutionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'Validation'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0merror_code\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: f71d81cd-40f3-4937-95cf-2f409f94e894\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://msftweudevcmnai2.dfs.core.windows.net/lake3?directory=projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '0d95535d-e628-4c0b-8188-0ec1c9b81c62', request ID '3bf2f524-f01f-000c-329c-66f7d8000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:3bf2f524-f01f-000c-329c-66f7d8000000\\nTime:2021-06-21T12:50:49.0460941Z\"}}\r\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mautomap_and_register_aml_datasets\u001b[1;34m(self, ws, dataset_name, dataset_description_in)\u001b[0m\n\u001b[0;32m   1623\u001b[0m                     \u001b[0min_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTabular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_parquet_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdstore_paths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create the Tabular dataset with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1624\u001b[1;33m                     \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregisterIn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdesc_in\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mregisterIn\u001b[1;34m(self, azure_dataset, description, new_version)\u001b[0m\n\u001b[0;32m   1975\u001b[0m         \u001b[0mds_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mazure_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_project\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_name_azure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_new_version\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1976\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mInData\u001b[1;34m(self, in_azure_dataset)\u001b[0m\n\u001b[0;32m   1934\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_project\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1935\u001b[1;33m             \u001b[0mdf_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1936\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;34m'Unnamed: 0'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Fix crap savings of CSV (using Spark CSV-driver or Excel, forgetting Index=False at save etc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\_loggerfactory.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\tabular_dataset.py\u001b[0m in \u001b[0;36mto_pandas_dataframe\u001b[1;34m(self, on_error, out_of_range_datetime)\u001b[0m\n\u001b[0;32m    170\u001b[0m                           \u001b[1;34m'to_pandas_dataframe'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                           None if self.id is None else {'id': self.id, 'name': self.name, 'version': self.version})\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0mfine_grain_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_properties\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_DATASET_PROP_TIMESTAMP_FINE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_try_execute\u001b[1;34m(action, operation, dataset_info, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_dprep_exception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_construct_message_and_check_exception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0m_dataprep_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_dprep_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\azure_automl\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py\u001b[0m in \u001b[0;36m_dataprep_error_handler\u001b[1;34m(e, message, is_dprep_exception)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_contains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'error_code'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mUserErrorException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minner_exception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUserErrorException\u001b[0m: UserErrorException:\n\tMessage: Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '0d95535d-e628-4c0b-8188-0ec1c9b81c62', request ID '3bf2f524-f01f-000c-329c-66f7d8000000'. Error message: [REDACTED]\r\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de ErrorCode: ScriptExecution.StreamAccess.NotFound\n\tInnerException \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: f71d81cd-40f3-4937-95cf-2f409f94e894\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://msftweudevcmnai2.dfs.core.windows.net/lake3?directory=projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '0d95535d-e628-4c0b-8188-0ec1c9b81c62', request ID '3bf2f524-f01f-000c-329c-66f7d8000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:3bf2f524-f01f-000c-329c-66f7d8000000\\nTime:2021-06-21T12:50:49.0460941Z\"}}\r\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\\r\\n  StreamAccessException was caused by NotFoundException.\\r\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '0d95535d-e628-4c0b-8188-0ec1c9b81c62', request ID '3bf2f524-f01f-000c-329c-66f7d8000000'. Error message: [REDACTED]\\r\\n| session_id=0a6f04e3-fe12-45ef-bf1d-2ad11577c5de ErrorCode: ScriptExecution.StreamAccess.NotFound\"\n    }\n}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a64b08288229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munregister_all_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# For DEMO purpose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdev_test_prod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"dev\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Note: This happens in the INIT() method of DriverAPI/CallingAppliation...not loading for every Scoring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self, ws_in)\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_suppress_logging\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1495\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomap_and_register_aml_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munregister_all_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jostrm\\OneDrive - Microsoft\\0_GIT\\2_My\\github\\azure-enterprise-scale-ml\\azure-enterprise-scale-ml\\esml\\common\\esml.py\u001b[0m in \u001b[0;36mautomap_and_register_aml_datasets\u001b[1;34m(self, ws, dataset_name, dataset_description_in)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1665\u001b[0m                     raise UserErrorException(\"Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team\" \\\n\u001b[1;32m-> 1666\u001b[1;33m                     \". \\n Tip_1: Since INFERENCE MODE=TRUE, check that your Ingestion-pipeline (Azure Data factory) \" + str_1 + str_2) from e2\n\u001b[0m\u001b[0;32m   1667\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mUserErrorException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUserErrorException\u001b[0m: UserErrorException:\n\tMessage: Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team. \n Tip_1: Since INFERENCE MODE=TRUE, check that your Ingestion-pipeline (Azure Data factory) to IN-folder is successful. Check in lake if: correct model_version=1 and correct date_folder\n Tip_2: Maybe data is deleted in lake? But dataset is still registered? Check here in the datalake with Storage Explorer: projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Error! Please check that Dataset name you provied in ESMLProject contructor also matches datalake-folder-names setup by your ESML core team. \\n Tip_1: Since INFERENCE MODE=TRUE, check that your Ingestion-pipeline (Azure Data factory) to IN-folder is successful. Check in lake if: correct model_version=1 and correct date_folder\\n Tip_2: Maybe data is deleted in lake? But dataset is still registered? Check here in the datalake with Storage Explorer: projects/project002/03_diabetes_model_reg/inference/1/ds02_other/in/dev/2021/06/08/\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "if(unregister_all_datasets):\n",
    "    p.unregister_all_datasets(ws) # For DEMO purpose\n",
    "\n",
    "p.init(ws) \n",
    "p.dev_test_prod = \"dev\"\n",
    "#Note: This happens in the INIT() method of DriverAPI/CallingAppliation...not loading for every Scoring"
   ]
  },
  {
   "source": [
    "# 2) Test an `existing` Model webservice (AKS)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2a) Lets simulate INFERENCE: data arives from CLIENT to a `DriverAPI` that itself will call `this ModelAPI` (AKS)\n",
    "Note: `Step 1 and 2 are out of scope` of from the MACHINE LEARNING solution (and ESML), other than...\n",
    " - `ESML` being the creator of AML pipeline - `Bronze_2_Gold` that you can reuse\n",
    " - `ESML lakedesign`, that should be used, if INFERENCE are to be stored/cached in the lake\n",
    "\n",
    "NOW - the scenario: \n",
    "\n",
    "- DriverAPI will:\n",
    "\n",
    "    - 1)Fetch missing datasources from different systems, to get all data sources ds01, ds02,ds03 & SAVE to INFERENCE/IN folder in the datalake\n",
    "        - Use the `ESML` lakedesign:  IF `inference_model_version=1` in `settings/project_specific/lake_settings.json` then...\n",
    "        - `ESML` will switch to write at `INFERENCE/v1/ds01/IN/2020/01/01` folder \n",
    "    - 2)Call `Bronze_2_Gold` batch pipeline (AML) - same pipeline that was used for training: `IN->Bronze->Silver->Gold`, except...\n",
    "        - `Bronze_2_Gold` pipeline will read from IN folder, but configured for \"INFERENCE\", read/write to `\"mirror\"` place in the datalake, that will `CACHE` online predicted results\n",
    "        - (LABEL values are missing of course)\n",
    "    - 3) `DriverAPI calls ModelAPI` (This AKS webservuce) with data to score -> `X_text`\n",
    "\n",
    "- `ESML` ModelAPI will:\n",
    "    - 4) `Score` the data, and return results, also `save the results to the datalake`\n",
    "        - if `p.rnd=True` nothing is stored.\n",
    "    - 5) `Connect the scored data` to a caller, and the individual scores to a identity (user/machine)\n",
    "        - GLOBALLY & ESML Managed: The scoring file gets a caller_id. \n",
    "            - You: Need to pass the called-guid as a parameter. \n",
    "                - Alternatively, you can have a column in the dataframe called `esml_caller_id_string`, and ESML will use that.\n",
    "        - LOCALLY: The rows in the dataframe has an identity. \n",
    "            - You: Here you need to have a column in the dataframe, such as `user_id`, to be able to connect each row to a scoring.\n",
    "\n",
    "\n",
    "Step 1 and 2 are out of scope of from the MACHINE LEARNING solution  \n",
    "We START this notebooke at step 3 - we have `X_test` in `GOLD_Validate` status "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "now = dt.datetime.now()\n",
    "folder = now.strftime('%Y_%m_%d') \n",
    "print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(unregister_all_datasets): # Create a GOLD dataset, and SPLIT it\n",
    "    df_01 = p.DatasetByName(\"ds01_diabetes\").Silver.to_pandas_dataframe()\n",
    "    ds_gold_v1 = p.save_gold(df_01)\n",
    "\n",
    "    label = \"Y\"\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Bronze_2_Gold done, we can fetch X_test\n",
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "source": [
    "### 2b) Call onlin ModelAPI - score 1 row (alt 1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import pandas as pd\n",
    "#3) `DriverAPI calls ModelAPI`\n",
    "\n",
    "keyvault = p.ws.get_default_keyvault()\n",
    "api_url = keyvault.get_secret(name=\"esml-dev-p02-m03-api\")\n",
    "api_key = keyvault.get_secret(name=\"esml-dev-p02-m03-apisecret\")  # esml-dev-p02-m03-apisecret\n",
    "\n",
    "df = ESMLProject.call_webservice_own_url(X_test, api_url,api_key) # rest call. STATIC method - simulate \"we dont need ESML for this\" - just an URL and KEY from DriverAPI\n",
    "df.head() # Print scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 2b) Call online ModelAPI - score all rows (alt 2) - `via ESML wrapper`\n",
    "- ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "- Note: We can also save a GUID/ID, `to see which CALLER/User-Guid the scoring is about` \n",
    "    - We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter in the call"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.inferenceModelVersion=1\n",
    "print(p.ScoredPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caller_user_id = '81965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\n",
    "\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "# 3) How to GET SCORE data? how to FILTER scored results?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "M03_GOLD_VALIDATE : (37, 11)\n",
      "X_test  (37, 10)\n",
      "y_test  (37,)\n",
      "Note: USING enterprise performance settings. (This can be overridden in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=True\n",
      "Note: Fetching keys automatically via workspace keyvault.\n",
      "Saving scoring to lake for project folder project002 and inference_model_version: 1 ...\n",
      "...\n",
      "\n",
      "Saved DATA to score successfully in LAKE, as file 'to_score_91965d9c-40ca-4e47-9723-5a608a32a0e4.parquet'\n",
      "Saved SCORED data in LAKE, as file 'scored_91965d9c-40ca-4e47-9723-5a608a32a0e4.parquet'\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        AGE       SEX       BMI        BP        S1        S2        S3  \\\n",
       "0  0.048974  0.050680  0.123131  0.083844 -0.104765 -0.100895 -0.069172   \n",
       "1  0.070769 -0.044642  0.069241  0.037939  0.021822  0.001504 -0.036038   \n",
       "2  0.056239  0.050680 -0.030996  0.008101  0.019070  0.021233  0.033914   \n",
       "3  0.016281 -0.044642  0.017506 -0.022885  0.060349  0.044406  0.030232   \n",
       "4  0.023546 -0.044642  0.110198  0.063187  0.013567 -0.032942 -0.024993   \n",
       "\n",
       "         S4        S5        S6      result  \n",
       "0 -0.002592  0.036646 -0.030072  243.130371  \n",
       "1  0.039106  0.077633  0.106617  293.235993  \n",
       "2 -0.039493 -0.029528 -0.059067   94.014447  \n",
       "3 -0.002592  0.037232 -0.001078  148.809757  \n",
       "4  0.020655  0.099240  0.023775  252.999922  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AGE</th>\n      <th>SEX</th>\n      <th>BMI</th>\n      <th>BP</th>\n      <th>S1</th>\n      <th>S2</th>\n      <th>S3</th>\n      <th>S4</th>\n      <th>S5</th>\n      <th>S6</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048974</td>\n      <td>0.050680</td>\n      <td>0.123131</td>\n      <td>0.083844</td>\n      <td>-0.104765</td>\n      <td>-0.100895</td>\n      <td>-0.069172</td>\n      <td>-0.002592</td>\n      <td>0.036646</td>\n      <td>-0.030072</td>\n      <td>243.130371</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.070769</td>\n      <td>-0.044642</td>\n      <td>0.069241</td>\n      <td>0.037939</td>\n      <td>0.021822</td>\n      <td>0.001504</td>\n      <td>-0.036038</td>\n      <td>0.039106</td>\n      <td>0.077633</td>\n      <td>0.106617</td>\n      <td>293.235993</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.056239</td>\n      <td>0.050680</td>\n      <td>-0.030996</td>\n      <td>0.008101</td>\n      <td>0.019070</td>\n      <td>0.021233</td>\n      <td>0.033914</td>\n      <td>-0.039493</td>\n      <td>-0.029528</td>\n      <td>-0.059067</td>\n      <td>94.014447</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.016281</td>\n      <td>-0.044642</td>\n      <td>0.017506</td>\n      <td>-0.022885</td>\n      <td>0.060349</td>\n      <td>0.044406</td>\n      <td>0.030232</td>\n      <td>-0.002592</td>\n      <td>0.037232</td>\n      <td>-0.001078</td>\n      <td>148.809757</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.023546</td>\n      <td>-0.044642</td>\n      <td>0.110198</td>\n      <td>0.063187</td>\n      <td>0.013567</td>\n      <td>-0.032942</td>\n      <td>-0.024993</td>\n      <td>0.020655</td>\n      <td>0.099240</td>\n      <td>0.023775</td>\n      <td>252.999922</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy()\n",
    "caller_user_id = '91965d9c-40ca-4e47-9723-5a608a32a0e4' # Connect the scoring to a caller/user, globally for all rows\n",
    "\n",
    "df = p.call_webservice(p.ws, X_test, caller_user_id, False) # Auto-fetch key from keyvault, 1stRowOnlye=False\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021_04_10\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "now = dt.datetime.now()\n",
    "date_filter = now.strftime('%Y_%m_%d') \n",
    "print(date_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"10\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets found in filter 1\n",
      "Example: How many rows in 1st dataset?  37\n",
      "All rows 37\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        AGE       SEX       BMI        BP        S1        S2        S3  \\\n",
       "0  0.048974  0.050680  0.123131  0.083844 -0.104765 -0.100895 -0.069172   \n",
       "1  0.070769 -0.044642  0.069241  0.037939  0.021822  0.001504 -0.036038   \n",
       "2  0.056239  0.050680 -0.030996  0.008101  0.019070  0.021233  0.033914   \n",
       "3  0.016281 -0.044642  0.017506 -0.022885  0.060349  0.044406  0.030232   \n",
       "4  0.023546 -0.044642  0.110198  0.063187  0.013567 -0.032942 -0.024993   \n",
       "\n",
       "         S4        S5        S6      result  \n",
       "0 -0.002592  0.036646 -0.030072  243.130371  \n",
       "1  0.039106  0.077633  0.106617  293.235993  \n",
       "2 -0.039493 -0.029528 -0.059067   94.014447  \n",
       "3 -0.002592  0.037232 -0.001078  148.809757  \n",
       "4  0.020655  0.099240  0.023775  252.999922  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AGE</th>\n      <th>SEX</th>\n      <th>BMI</th>\n      <th>BP</th>\n      <th>S1</th>\n      <th>S2</th>\n      <th>S3</th>\n      <th>S4</th>\n      <th>S5</th>\n      <th>S6</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.048974</td>\n      <td>0.050680</td>\n      <td>0.123131</td>\n      <td>0.083844</td>\n      <td>-0.104765</td>\n      <td>-0.100895</td>\n      <td>-0.069172</td>\n      <td>-0.002592</td>\n      <td>0.036646</td>\n      <td>-0.030072</td>\n      <td>243.130371</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.070769</td>\n      <td>-0.044642</td>\n      <td>0.069241</td>\n      <td>0.037939</td>\n      <td>0.021822</td>\n      <td>0.001504</td>\n      <td>-0.036038</td>\n      <td>0.039106</td>\n      <td>0.077633</td>\n      <td>0.106617</td>\n      <td>293.235993</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.056239</td>\n      <td>0.050680</td>\n      <td>-0.030996</td>\n      <td>0.008101</td>\n      <td>0.019070</td>\n      <td>0.021233</td>\n      <td>0.033914</td>\n      <td>-0.039493</td>\n      <td>-0.029528</td>\n      <td>-0.059067</td>\n      <td>94.014447</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.016281</td>\n      <td>-0.044642</td>\n      <td>0.017506</td>\n      <td>-0.022885</td>\n      <td>0.060349</td>\n      <td>0.044406</td>\n      <td>0.030232</td>\n      <td>-0.002592</td>\n      <td>0.037232</td>\n      <td>-0.001078</td>\n      <td>148.809757</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.023546</td>\n      <td>-0.044642</td>\n      <td>0.110198</td>\n      <td>0.063187</td>\n      <td>0.013567</td>\n      <td>-0.032942</td>\n      <td>-0.024993</td>\n      <td>0.020655</td>\n      <td>0.099240</td>\n      <td>0.023775</td>\n      <td>252.999922</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#df = p.get_scored('last_month')\n",
    "ds_list1, df_all1 = p.get_scored(date_filter, \"1\",'81965d9c-40ca-4e47-9723-5a608a32a0e4')\n",
    "print(\"Datasets found in filter\", len(ds_list1))\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list1[0].to_pandas_dataframe().shape[0])\n",
    "print(\"All rows\", df_all1.shape[0])\n",
    "\n",
    "df_all1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Datasets found in filter 2\n",
      "Example: How many rows in 1st dataset?  37\n",
      "All rows 74\n"
     ]
    }
   ],
   "source": [
    "ds_list, df_all = p.get_scored(date_filter, \"1\")\n",
    "print(\"Datasets found in filter\", len(ds_list))\n",
    "print(\"Example: How many rows in 1st dataset? \", ds_list[0].to_pandas_dataframe().shape[0])\n",
    "print(\"All rows\", df_all.shape[0])"
   ]
  },
  {
   "source": [
    " ** Train model & register ....this happended already in another notebook/MLops pipline step\n",
    "> model = p.register_active_model()  \n",
    "> print(model.name, model.description, model.version)  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3a) ESML `Deploy a new ONLINE` webservice (AKS)\n",
    "- Deploy \"offline\" from old `AutoML run` for `DEV` environment\n",
    "- To   `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(ws) #  AutoML support \n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config, True) # overwrite_endpoint=True"
   ]
  },
  {
   "source": [
    "# Test the NEW AKS WebService"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() # Version is default latest\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.call_webservice(p.ws, X_test) # Auto-fetch key from keyvault, 1stRowOnlye=False  |   p.call_webservice(p.ws, X_test, caller_user_id, False)\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "# Code to `embed in YOUR DriverApi` - to call this webservice `without ESML`\n",
    "- `NB!` This will `NOT cache` the results automatically, since not going via the ESML SDK. You need to save the scoring yourself ( if this is needed)\n",
    "    - ESML benefits: Has built in `logic` to save scored_results, to unique folders during the day.\n",
    "            # Note: We can also save a GUID/ID, for which CALLER/User-Guid it is about. We can have a User_id_GUID as a \"feature/column\" in X_test, or as a parameter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_webservice_code(rows, api_uri,api_key, allowSelfSigned=True):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowSelfSigned and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    X_test_json_works = json.dumps({'data': rows.to_dict(orient='records')})\n",
    "\n",
    "    headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + api_key}\n",
    "    resp = requests.post(api_uri, X_test_json_works , headers=headers) \n",
    "\n",
    "    # Here you can pass forward the response, save it to the datalake, or as below return a PANDAS dataframe\n",
    "    res_dict = json.loads(resp.text)\n",
    "    res_dict_ast = ast.literal_eval(res_dict)\n",
    "    return pd.read_json(res_dict) # to pandas"
   ]
  },
  {
   "source": [
    "## 3b) ESML `Deploy BATCH` pipeline\n",
    "- Deploy same model \"offline / previous\" `AutoML Run` for `DEV` environment\n",
    "- To   `DEV`, `TEST` or `PROD` environment\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5a alt2) If not using AutoML - you need to manuallt create `environemnt + entryscript + inference config`\n",
    "https://github.com/Azure/MachineLearningNotebooks/blob/bda592a236eaf2dbc54b394e1fa1b539e0297908/how-to-use-azureml/deployment/production-deploy-to-aks/production-deploy-to-aks.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ENVIRONMENT - If not AUTOML\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1','scipy'], pip_packages=['azureml-defaults', 'inference-schema'])\n",
    "myenv = Environment(name='myenv')\n",
    "myenv.python.conda_dependencies = conda_deps"
   ]
  },
  {
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_regression_model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "inf_config = InferenceConfig(entry_script='score.py', environment=myenv)"
   ]
  },
  {
   "source": [
    "# OTHER - What more can you retrieve from AutoMLFactory and ESMLProject?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import AutoMLFactory\n",
    "from baselayer_azure_ml import ComputeFactory\n",
    "\n",
    "target_model, target_best_run_id = AutoMLFactory().get_latest_model(ws)\n",
    "print(target_model.name)\n",
    "print(target_best_run_id)\n",
    "\n",
    "run, exp = p.get_active_model_run_and_experiment()\n",
    "inference_config = p.get_active_model_inference_config()\n",
    "print(run)\n",
    "print(exp.name)"
   ]
  }
 ]
}