{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ESML - TRAIN Classification, TITANIC model, and DEPLOY with predict_proba scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\n",
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "#p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "######  NB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\n",
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "#p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "######  NB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "import pandas as pd\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\n",
    "p.active_model = 10\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\n",
    "p.inference_mode = False\n",
    "\n",
    "unregister_all_datasets=False\n",
    "if(unregister_all_datasets):\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose\n",
    "\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_engieering():\n",
    "    # R&D purpose: Try some data wrangling here...we will later incorporate this in an Azure ML Pipeline, as \"steps\"\n",
    "    esml_dataset = p.DatasetByName(\"ds01_titanic\") \n",
    "    df_bronze = esml_dataset.Bronze.to_pandas_dataframe()\n",
    "    df_bronze.columns = df_bronze.columns.str.replace(\"[/]\", \"_\") # Rename werid column names\n",
    "\n",
    "    df_silver = p.save_silver(esml_dataset,df_bronze) #Bronze -> Silver\n",
    "\n",
    "    esml_dataset2 = p.DatasetByName(\"ds02_haircolor\")\n",
    "    esml_dataset3 = p.DatasetByName(\"ds03_housing\")\n",
    "    esml_dataset4 = p.DatasetByName(\"ds04_lightsaber\")\n",
    "\n",
    "    p.save_silver(esml_dataset2,esml_dataset2.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "    p.save_silver(esml_dataset3,esml_dataset3.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "    p.save_silver(esml_dataset4,esml_dataset4.Bronze.to_pandas_dataframe()) #Bronze -> Silver\n",
    "\n",
    "    gold = p.save_gold(esml_dataset.Silver.to_pandas_dataframe())  #Silver -> Gold STEP\n",
    "    return gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = None\n",
    "gold = None\n",
    "gold_train = None\n",
    "try:\n",
    "    datastore = p.connect_to_lake()\n",
    "    gold = p.Gold\n",
    "    gold_train = p.GoldTrain\n",
    "    gold_train.name\n",
    "    print(\"Not 1st time. We have data mapped already. Now connected to LAKE\")\n",
    "except: # If 1st time....no Gold exists, nor any mapping\n",
    "    print(\"1st time. Lets init, map what data we have in LAKE, as Azure ML Datasets\")\n",
    "    datastore = p.init() # 3) Automapping from datalake to Azure ML datasets\n",
    "    gold = test_feature_engieering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.Gold.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY - step 1\n",
    "- ESML has now `Automap` and `Autoregister` Azure ML Datasets as: `IN, SILVER, BRONZE, GOLD`\n",
    "- ESML has read configuration for correct environment (DEV, TEST, PROD). \n",
    "    - Both small customers, and large Enterprise customers often wants:  DEV, TEST, PROD in `diffferent Azure ML workspaces` (and different subscriptions)\n",
    "- User has done feature engineering, and saved GOLD `p.save_gold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Production purpose: \"once and only once\": Wrap code\n",
    "- 3 Callers: MLOps, AMLPipeline, and this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../2_A_aml_pipeline/4_inference/batch/M10/your_code/\")\n",
    "from your_custom_code import M01In2GoldProcessor\n",
    "\n",
    "#p.init()\n",
    "esml_dataset1 = p.DatasetByName(\"ds01_titanic\") # Get dataset 1\n",
    "df_bronze = esml_dataset1.Bronze.to_pandas_dataframe()\n",
    "silver1 = p.save_silver(esml_dataset1,df_bronze) #Bronze -> Silver\n",
    "\n",
    "esml_dataset2 = p.DatasetByName(\"ds02_haircolor\") # Get dataset 2\n",
    "df_bronze2 = esml_dataset2.Bronze.to_pandas_dataframe()\n",
    "silver2 = p.save_silver(esml_dataset2,df_bronze2) #Bronze -> Silver\n",
    "\n",
    "df1 = M01In2GoldProcessor().M01_ds01_process_in2silver(silver1.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds01...py\"\n",
    "df2 = M01In2GoldProcessor().M01_ds02_process_in2silver(silver2.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds02...py\"\n",
    "\n",
    "merged_gold = M01In2GoldProcessor().M01_merge_silvers(df1,df2) # # You can then copy this statement in your pipeline-step \"silver_merged_2_gold.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label) # Auto-register datasets in AZURE (GOLD_TRAIN | GOLD_VALIDATE | GOLD_TEST)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `ESML` Train model in `5 codelines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esml import ESMLDataset, ESMLProject\n",
    "from baselayer_azure_ml import AutoMLFactory,azure_metric_regression,azure_metric_classification\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "aml_compute = p.get_training_aml_compute(p.ws) # 2)Get compute, for active environment\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification', # 4) Override the ENV config, for model(that inhertits from enterprise DEV_TEST_PROD config baseline)\n",
    "                            primary_metric = azure_metric_classification.AUC, # # Note: Regression(MAPE) are not possible in AutoML\n",
    "                            compute_target = aml_compute,\n",
    "                            training_data = p.GoldTrain, # is 'train_6' pandas dataframe, but as an Azure ML Dataset\n",
    "                            experiment_exit_score = '0.922', # DEMO purpose (0.308 for diabetes regression, 0.6 for classification titanic)\n",
    "                            label_column_name = label,\n",
    "                            **automl_performance_config\n",
    "                        )\n",
    "via_pipeline = False # Consistent/same return values from both AutoML ALTERNATIVES (run or pipeline)\n",
    "best_run, fitted_model, experiment = AutoMLFactory(p).train_pipeline(automl_config) if via_pipeline else AutoMLFactory(p).train_as_run(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ESML has now fetched `configuration & train compute` for enterprise `environment (DEV,TEST or PROD)`\n",
    "- ESML has `autogenerated` a AutoML-experiment, optinally as `pipline`, in correct environment.\n",
    "- User has overridden some AutoML settings (`label, split percentage`, `target metric`), and use the `1-liner TRAIN` code snippet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) ESML Scoring Drift/Concept Drift: Compare with `1-codeline`: Promote model or not? If better, then `Register model`\n",
    "- `IF` newly trained model in `current` environment (`DEV`, `TEST` or `PROD`) scores BETTER than existing model in `target` environment, then `new model` can be registered and promoted.\n",
    "- Q: Do we have `SCORING DRIFT / CONCEPT DRIFT?`\n",
    "- Q: Is a model trained on NEW data better? IS the one in production degraded? (not fit for the data it scores - real world changed, other CONCEPT)\n",
    "- A: - Lets check. Instead of `DataDrift`, lets look at `actual SCORING` on new data (and/or new code, feature engineering) - See if we should PROMOTE newly trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml_model import ESMLModelCompare\n",
    "\n",
    "current_env = p.dev_test_prod # dev\n",
    "target_env = \"dev\" # Does newly trained Model v3 in DEV, score better than Model v2 in TEST?\n",
    "print(\"promote model in DEV to TEST? (move to other Azure ML Studio Workspace)\")\n",
    "\n",
    "compare = ESMLModelCompare(p)\n",
    "promote,source_model_name,new_run_id,target_model_name, target_best_run_id,target_workspace,source_model = compare.compare_scoring_current_vs_new_model(target_env) # Compare DEV to TEST (or TEST to PROD)  (1min, 17sek VS 33sec)\n",
    "\n",
    "print(\"SCORING DRIFT: If new model scores better in DEV (new data, or new code), we can promote this to TEST & PROD \\n\")\n",
    "print(\"New Model: {} in environment {}\".format(target_model_name, p.dev_test_prod))\n",
    "print(\"Existing Model: {} in environment {}\".format(source_model_name,target_env))\n",
    "\n",
    "if (promote): # Can register=\"promote\" a model in same workspace (test->test), or also register in OTHER Azure ML workspace (test->prod)\n",
    "    if(p.dev_test_prod == target_env):\n",
    "        compare.register_active_model(target_env,source_model) # if SAME workspace this brings more \"metadata\" faster to the model registration\n",
    "    else:\n",
    "        compare.register_model_in_correct_ws(target_env) # if REMOTE target workspace we can get same metadata, BUT, just takes performancewise longer. More lookups to \"source Run\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SET SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-set: Ensure we have a TEST_SET splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "try:\n",
    "    p.GoldTest.name\n",
    "except: \n",
    "    p.connect_to_lake()\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW we can calcualate scoring on TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import ESMLTestScoringFactory\n",
    "label = p.active_model[\"label\"]\n",
    "\n",
    "auc,accuracy,f1, precision,recall,matrix,matthews, plt = ESMLTestScoringFactory(p).get_test_scoring_7_classification(label)\n",
    "\n",
    "print(\"AUC:\")\n",
    "print(auc)\n",
    "print()\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print()\n",
    "print(\"F1 Score:\")\n",
    "print(f1)\n",
    "print()\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print()\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print()\n",
    "print(\"Matchews correlation:\")\n",
    "print(matthews)\n",
    "print()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ESML `Deploy model ONLINE` in `2 lines of code` (AKS) \n",
    "- Deploy \"offline\" MODEL from old `run` in environment To â†’  `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(p.ws) #  Get compute power & lib-dependecies for DOCKER...for correct (Dev,Test or Prod) environment.\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config,True) # Deploy: AKS dockerized with correct config (Dev,Test or Prod subscription & networking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) DEPLOY TEST with ESML `2 lines of code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() \n",
    "print(tags)\n",
    "caller_id = \"10965d9c-40ca-4e47-9723-5a608a32a0e4\"\n",
    "\n",
    "df = p.call_webservice(p.ws, X_test,caller_id) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b) ESML `DEPLOY - custom scoring` file - predict proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(globals()['_dh'][0]))\n",
    "\n",
    "my_custom_script_instead = 'scoring_file_dev_M01_titanic.py'\n",
    "script_file_local = \"./settings/project_specific/model/dev_test_prod/train/automl/\"+my_custom_script_instead\n",
    "script_file_abs = os.path.abspath(script_file_local)\n",
    "\n",
    "inference_config_to_override_and_inject, model, best_run = p.get_active_model_inference_config(p.ws)\n",
    "inference_config_to_override_and_inject.entry_script = script_file_abs\n",
    "inference_config_to_override_and_inject.entry_script # Verify path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY with custom InferenceConfig (custom scoring script)\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config_to_override_and_inject, True) #2) (model,inference_config, overwrite_endpoint=True,deployment_config=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE - Scenario \"Caller/Client\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get MockData - Get some TEST-DATA via ESMLProject...the GoldTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\n",
    "p.inference_mode = False # We want \"TRAIN\" mode\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "to_score = None\n",
    "try:\n",
    "    X_test = p.GoldTest.to_pandas_dataframe()\n",
    "    to_score = X_test.drop([label], axis=1)\n",
    "    #print(to_score.head()) # gold_test_1 = Dataset.get_by_name(ws, name=p.dataset_gold_test_name_azure)\n",
    "except: \n",
    "    print (\"you need to have splitted GOLD dataset, GoldTest need to exist. Change next cell from MARKDOWN, to CODE, and run that. Try this again... \")\n",
    "# #X_test, y_test, tags = p.get_gold_validate_Xy() # Get the X_test data, ESML knows the SPLIT and LABEL already (due to training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Call AKS Webservice in 3 ways (A,B,C)\n",
    "- A) Also let AKS save data to lake\n",
    "- B) Use the ESML helper method (fetched keys from vault AND joins result + features)\n",
    "- C) Simulate \"Rest only\" - No ESML dependency \n",
    "    - No ESML meaning: Fetch keys by your own from vault + join/format JSON yourself + save data yourself to lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 1 - ESML.call_webservice, `get PANDAS joined` dataframe\n",
    "#### `Also saves to LAKE, automatically`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.lakestore = p.set_lake_as_datastore(p.ws) # For AutoSave - this i NOT needed if p.init() is done...which usually is the case.\n",
    "p.call_webservice(p.ws, to_score,\"caller_id\").head() # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 2 - use compute factory, control to `get JSON back` instead of PANDAS. \n",
    "#### `No saving to LAKE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_version_used = p.compute_factory.call_webservice(to_score,False,False) # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")\n",
    "df_res = pd.read_json(result)\n",
    "to_score.join(df_res) # Need to join the FEATURES yourself, post webservice call (simulate no ESML dependancy in caller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 3 - Simulate client witn no ESML SDK, just using the \"scoring endpoint\". \n",
    "- Just JSON result (No ESML dependancy `get JSON back`)\n",
    "#### `No saving to LAKE` and `no JOIN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import ComputeFactory\n",
    "import json\n",
    "keyvault = p.ws.get_default_keyvault() # Authentica to your Azure ML workspace (ws)\n",
    "api_uri = keyvault.get_secret(name='esml-dev-p02-m10-api') \n",
    "api_key = keyvault.get_secret(name='esml-dev-p02-m10-apisecret') # DEV + Titanic\n",
    "\n",
    "#api_uri = keyvault.get_secret(name='esml-test-p02-m10-api') # TEST + Titanic\n",
    "#api_key = keyvault.get_secret(name='esml-test-p02-m10-apisecret')\n",
    "\n",
    "result_json = ComputeFactory.call_webservice_static(to_score, api_uri,api_key,firstRowOnly=False) # Simulate \"REST call\" (no ESML dependancy, just a wrapper for a pytnon REST call)\n",
    "res_dict = json.loads(result_json.text) # json -> dictionary\n",
    "df_res = pd.read_json(res_dict) # dictionary -> pandas\n",
    "all_result = X_test.join(df_res) # features + result\n",
    "all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And....you need to save the data yourself to the lake, at this location\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\n",
    "print(\"Save your data here, if you want to have ADF WriteBack function\")\n",
    "print()\n",
    "print(scored_folder)\n",
    "print()\n",
    "print(\"Note: Last folder, UUID folder, should represent a 'unique scoring' for a day, but can be injected. Example: if we want a customerGUID instead \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA - more about `AutoLake Paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "p = ESMLProject() \n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.inference_mode = True # This flag will \"change the paths\", from TRAIN folder to INFERENCE folder-structure\n",
    "\n",
    "print(\"\")\n",
    "print(\"INFERENCE\")\n",
    "print(\"\")\n",
    "\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    print(\"IN\", d.InPath)\n",
    "    print(\"Bronze\", d.BronzePath)\n",
    "    print(\"Silver\", d.SilverPath)\n",
    "\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)\n",
    "print(\"Gold\", to_score_folder, \"  ...uuid folder, is to be able to have multiple unique scorings, same datetime\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TRAIN\")\n",
    "print(\"\")\n",
    "\n",
    "p.inference_mode = False # This flag will \"change the paths\"\n",
    "\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    print(\"IN\", d.InPath)\n",
    "    print(\"Bronze\", d.BronzePath)\n",
    "    print(\"Silver\", d.SilverPath)\n",
    "\n",
    "print(\"Gold\", p.GoldPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
