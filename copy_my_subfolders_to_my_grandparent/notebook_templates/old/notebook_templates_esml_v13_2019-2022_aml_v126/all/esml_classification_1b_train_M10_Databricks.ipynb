{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ESML - TRAIN Classification, TITANIC model, and DEPLOY with predict_proba scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\n",
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "#p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "######  NB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  NB! This,InteractiveLoginAuthentication, is only needed to run 1st time, then when ws_config is written, use later CELL in notebook, that just reads that file\n",
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "#sys.path.append(os.path.abspath(\"../azure-enterprise-scale-ml/esml/common/\"))  # NOQA: E402\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "#p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)\n",
    "######  NB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "import pandas as pd\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\n",
    "p.active_model = 10\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\n",
    "p.inference_mode = False\n",
    "\n",
    "unregister_all_datasets=False\n",
    "if(unregister_all_datasets):\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose\n",
    "\n",
    "p.connect_to_lake()\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global esml_dataset_titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_feature_engieering():\n",
    "    global esml_dataset_titanic\n",
    "    \n",
    "    esml_dataset_titanic = p.DatasetByName(\"ds01_titanic\") # Get dataset\n",
    "    df_bronze = esml_dataset_titanic.InData.to_pandas_dataframe()\n",
    "    return df_bronze\n",
    "before_feature_engieering().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"This is the Python/Pandas .parqut-representation of Bronze, from VS Code/Python. Not Databricks/PySpark .parquet\")\n",
    "print(\"Look at the column name of 'Siblings_Spouses_Aboard'\")\n",
    "esml_dataset_titanic.Bronze.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"This is the Databricks/PySpark .parquet representation - Another folder in datalake, partitioned parquet files that Databricks/PySpark generated\")\n",
    "print(\"Look at the column name of 'Siblings_#_Spouses_Aboard'\")\n",
    "esml_dataset_titanic.SilverDatabricks.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Get Databricks GOLD`: Alternative 1 \n",
    "- Use ESML property to get PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from azureml.core import Dataset\n",
    " gold1 = Dataset.Tabular.from_parquet_files(path = [(p.Lakestore, p.GoldPathDatabricks)])\n",
    " gold1.to_pandas_dataframe().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Get Databricks GOLD`: Alternative 2\n",
    "- Use ESML property with Azure ML Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.GoldDatabricks.to_pandas_dataframe().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we want to `SPLIT Databricks GOLD`, and `Train` our model.\n",
    "## Q: Why? Benefits: \"Best of both worlds\"\n",
    "### Scenario 01: In the `R&D phase` we worked in Databricks to featuer engineer our data - NOW we want to TRAIN a model with `Azure ML (CPU clusters) & ESML` - 3 lines of code & AutoML\n",
    "### Scenario 02: In the `Production phase`: We want to use Databricks to featuer engineer our data - But TRAIN a model with  `Azure ML (CPU clusters) & ESML` - easy deployment/Pipeline-generation and lineage\n",
    "\n",
    "### A) Databricks/Spark clusters, with PySpark to `crunch a lot data, very fast`\n",
    "### B) Azure ML CPU clusters, with Python to get the below benefits:\n",
    "- `Azure ML Datasets` with Datadrift, Lineage, Versioning/Timetravel\n",
    "- `ESML MLOps & Scoring drift`\n",
    "- `ESML TEST_SET calculation`\n",
    "- `ESML PipelineFactory (Azure ML pipeline)`\n",
    "- `ESML 1-line deploy model on AKS for ONLINE webservice`\n",
    "- `ESML Datalake design for INFERENCE/SCORING`\n",
    "- `Azure ML Studio: Deploy model on AKS online, or as a BATCH pipeline`\n",
    "- `Azure ML / AutoML` compatibility\n",
    "- `Python` compatibility (PySpark is sensitive,e.g. if it does not support your favourite libraries to train ML models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databricks_gold_and_split_it_as_AzureMLDatasets():\n",
    "    label = p.active_model[\"label\"]\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_dbx(0.6,label) \n",
    "\n",
    "    return p.GoldDatabricks,train_6, validate_set_2, test_set_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = None\n",
    "gold = None\n",
    "gold_train = None\n",
    "try:\n",
    "    datastore = p.connect_to_lake()\n",
    "    gold = p.GoldDatabricks\n",
    "    gold_train = p.GoldTrain\n",
    "    gold_train.name\n",
    "    print(\"Not 1st time. We have data mapped already. Now connected to LAKE\")\n",
    "except: # If 1st time....no Gold exists, nor any mapping\n",
    "    print(\"1st time. Lets init, map what data we have in LAKE, as Azure ML Datasets\")\n",
    "    datastore = p.init() # 3) Automapping from datalake to Azure ML datasets\n",
    "    gold,train_6, validate_set_2, test_set_2 = get_databricks_gold_and_split_it_as_AzureMLDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Lets SPLIT the Databricks GOLD: Train,Validate, Test`\n",
    "- To update from the Databricks GOLD, is the basis of the TRAIN, VALIDATE, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold,train_6, validate_set_2, test_set_2 = get_databricks_gold_and_split_it_as_AzureMLDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.Gold.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.GoldDatabricks.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  We have now MARRIED the world of `Azure ML Datasets + Azure Databricks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY - step 1\n",
    "- ESML has read configuration for correct environment (DEV, TEST, PROD). \n",
    "- ESML has now `Automap` and `Autoregister` Azure ML Datasets as: `IN, SILVER, BRONZE, GOLD`\n",
    "- ESML has read read `Databricks GOLD`, and splitted it to Gold_Train, Gold_Validate, Gold_Test, and auto-registered them as Azure ML Datasets\n",
    "- ...We are now ready to TRAIN our model. We needed an Azure ML Dataset for that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Production purpose: \"once and only once\": Wrap code\n",
    "- 3 Callers: MLOps, AMLPipeline, and this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../2_A_aml_pipeline/4_inference/batch/M10/your_code/\")\n",
    "from your_custom_code import M01In2GoldProcessor\n",
    "\n",
    "#p.init()\n",
    "esml_dataset1 = p.DatasetByName(\"ds01_titanic\") # Get dataset 1\n",
    "df_bronze = esml_dataset1.Bronze.to_pandas_dataframe()\n",
    "silver1 = p.save_silver(esml_dataset1,df_bronze) #Bronze -> Silver\n",
    "\n",
    "esml_dataset2 = p.DatasetByName(\"ds02_haircolor\") # Get dataset 2\n",
    "df_bronze2 = esml_dataset2.Bronze.to_pandas_dataframe()\n",
    "silver2 = p.save_silver(esml_dataset2,df_bronze2) #Bronze -> Silver\n",
    "\n",
    "df1 = M01In2GoldProcessor().M01_ds01_process_in2silver(silver1.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds01...py\"\n",
    "df2 = M01In2GoldProcessor().M01_ds02_process_in2silver(silver2.to_pandas_dataframe())  # You can then copy this statement in your pipeline-step \"in2silver_ds02...py\"\n",
    "\n",
    "merged_gold = M01In2GoldProcessor().M01_merge_silvers(df1,df2) # # You can then copy this statement in your pipeline-step \"silver_merged_2_gold.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label) # Auto-register datasets in AZURE (GOLD_TRAIN | GOLD_VALIDATE | GOLD_TEST)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `ESML` Train model in `5 codelines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esml import ESMLDataset, ESMLProject\n",
    "from baselayer_azure_ml import AutoMLFactory,azure_metric_regression,azure_metric_classification\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "aml_compute = p.get_training_aml_compute(p.ws) # 2)Get compute, for active environment\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification', # 4) Override the ENV config, for model(that inhertits from enterprise DEV_TEST_PROD config baseline)\n",
    "                            primary_metric = azure_metric_classification.AUC, # # Note: Regression(MAPE) are not possible in AutoML\n",
    "                            compute_target = aml_compute,\n",
    "                            training_data = p.GoldTrain, # is 'train_6' pandas dataframe, but as an Azure ML Dataset\n",
    "                            experiment_exit_score = '0.922', # DEMO purpose (0.308 for diabetes regression, 0.6 for classification titanic)\n",
    "                            label_column_name = label,\n",
    "                            **automl_performance_config\n",
    "                        )\n",
    "via_pipeline = False # Consistent/same return values from both AutoML ALTERNATIVES (run or pipeline)\n",
    "best_run, fitted_model, experiment = AutoMLFactory(p).train_pipeline(automl_config) if via_pipeline else AutoMLFactory(p).train_as_run(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ESML has now fetched `configuration & train compute` for enterprise `environment (DEV,TEST or PROD)`\n",
    "- ESML has `autogenerated` a AutoML-experiment, optinally as `pipline`, in correct environment.\n",
    "- User has overridden some AutoML settings (`label, split percentage`, `target metric`), and use the `1-liner TRAIN` code snippet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) ESML Scoring Drift/Concept Drift: Compare with `1-codeline`: Promote model or not? If better, then `Register model`\n",
    "- `IF` newly trained model in `current` environment (`DEV`, `TEST` or `PROD`) scores BETTER than existing model in `target` environment, then `new model` can be registered and promoted.\n",
    "- Q: Do we have `SCORING DRIFT / CONCEPT DRIFT?`\n",
    "- Q: Is a model trained on NEW data better? IS the one in production degraded? (not fit for the data it scores - real world changed, other CONCEPT)\n",
    "- A: - Lets check. Instead of `DataDrift`, lets look at `actual SCORING` on new data (and/or new code, feature engineering) - See if we should PROMOTE newly trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import AutoMLFactory\n",
    "target_env = p.dev_test_prod #\"dev\", test, prod  = Target environment. Does Model A score better than Model B?\n",
    "print(\"SCORING DRIFT: If new model scores better in DEV (new data, or new code), we can promote this to TEST & PROD \\n\")\n",
    "\n",
    "promote, m1_name, r1_id, m2_name, r2_run_id = AutoMLFactory(p).compare_scoring_current_vs_new_model(target_env)  \n",
    "\n",
    "# To override (option 1) \"definition of BETTER\" - Adjust settings/model_settings.json to define & adjust \"what is best in my use case\"\n",
    "# To override (option 2) - just Register whatever model you see is the best. \"latest registered = best\"\n",
    "# - After, if you want to use AutoMLFactory(p).register_active_model(target_env), need to edit 'run_id' 'registered_model_version' in  /project_specifics/dev_test_prod/train/.../automl_active_model_dev.json\n",
    "\n",
    "# To override (option 3) - inject your own LAMBDA function \"your definition iof best\" \n",
    "#my_def_of_what_model_is_better = lambda sklearn_model_new,sklearn_model_current : (sklearn_model_new > sklearn_model_current)\n",
    "#promote, m1_name, r1_id, m2_name, r2_run_id = AutoMLFactory(p).compare_scoring_current_vs_new_model(target_env,my_def_of_what_model_is_better)\n",
    "\n",
    "print(\"New Model: {} in environment {}\".format(m1_name, p.dev_test_prod))\n",
    "print(\"Existing Model: {} in environment {}\".format(m2_name,target_env))\n",
    "\n",
    "if (promote and p.dev_test_prod == target_env): # Can register a model in same workspace (test->test) - need to retrain if going from dev->test (but copy from test->prod)\n",
    "    AutoMLFactory(p).register_active_model(target_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST SET SCORING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-set: Ensure we have a TEST_SET splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "try:\n",
    "    p.GoldTest.name\n",
    "except: \n",
    "    p.connect_to_lake()\n",
    "    train_6, validate_set_2, test_set_2 = p.split_gold_3(0.6,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOW we can calcualate scoring on TEST_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import ESMLTestScoringFactory\n",
    "label = p.active_model[\"label\"]\n",
    "\n",
    "auc,accuracy,f1, precision,recall,matrix,matthews, plt = ESMLTestScoringFactory(p).get_test_scoring_7_classification(label)\n",
    "\n",
    "print(\"AUC:\")\n",
    "print(auc)\n",
    "print()\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print()\n",
    "print(\"F1 Score:\")\n",
    "print(f1)\n",
    "print()\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print()\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print()\n",
    "print(\"Matchews correlation:\")\n",
    "print(matthews)\n",
    "print()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) ESML `Deploy model ONLINE` in `2 lines of code` (AKS) \n",
    "- Deploy \"offline\" MODEL from old `run` in environment To â†’  `DEV`, `TEST` or `PROD` environment\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference_config, model, best_run = p.get_active_model_inference_config(p.ws) #  Get compute power & lib-dependecies for DOCKER...for correct (Dev,Test or Prod) environment.\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config,True) # Deploy: AKS dockerized with correct config (Dev,Test or Prod subscription & networking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) DEPLOY TEST with ESML `2 lines of code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy() \n",
    "print(tags)\n",
    "caller_id = \"10965d9c-40ca-4e47-9723-5a608a32a0e4\"\n",
    "\n",
    "df = p.call_webservice(p.ws, X_test,caller_id) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b) ESML `DEPLOY - custom scoring` file - predict proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(globals()['_dh'][0]))\n",
    "\n",
    "my_custom_script_instead = 'scoring_file_dev_M01_titanic.py'\n",
    "script_file_local = \"./settings/project_specific/model/dev_test_prod/train/automl/\"+my_custom_script_instead  # HERE you customize the auto-generated scoring script\n",
    "script_file_abs = os.path.abspath(script_file_local)\n",
    "\n",
    "inference_config_to_override_and_inject, model, best_run = p.get_active_model_inference_config(p.ws)\n",
    "inference_config_to_override_and_inject.entry_script = script_file_abs\n",
    "inference_config_to_override_and_inject.entry_script # Verify path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY with custom InferenceConfig (custom scoring script)\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_automl_model_to_aks(model,inference_config_to_override_and_inject, True) #2) (model,inference_config, overwrite_endpoint=True,deployment_config=None):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE - Scenario \"Caller/Client\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Get MockData - Get some TEST-DATA via ESMLProject...the GoldTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../../../settings', you should copy template settings from '../settings'\n",
    "p.inference_mode = False # We want \"TRAIN\" mode\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = p.active_model[\"label\"]\n",
    "to_score = None\n",
    "try:\n",
    "    X_test = p.GoldTest.to_pandas_dataframe()\n",
    "    to_score = X_test.drop([label], axis=1)\n",
    "    #print(to_score.head()) # gold_test_1 = Dataset.get_by_name(ws, name=p.dataset_gold_test_name_azure)\n",
    "except: \n",
    "    print (\"you need to have splitted GOLD dataset, GoldTest need to exist. Change next cell from MARKDOWN, to CODE, and run that. Try this again... \")\n",
    "# #X_test, y_test, tags = p.get_gold_validate_Xy() # Get the X_test data, ESML knows the SPLIT and LABEL already (due to training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Call AKS Webservice in 3 ways (A,B,C)\n",
    "- A) Also let AKS save data to lake\n",
    "- B) Use the ESML helper method (fetched keys from vault AND joins result + features)\n",
    "- C) Simulate \"Rest only\" - No ESML dependency \n",
    "    - No ESML meaning: Fetch keys by your own from vault + join/format JSON yourself + save data yourself to lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 1 - ESML.call_webservice, `get PANDAS joined` dataframe\n",
    "#### `Also saves to LAKE, automatically`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.lakestore = p.set_lake_as_datastore(p.ws) # For AutoSave - this i NOT needed if p.init() is done...which usually is the case.\n",
    "p.call_webservice(p.ws, to_score,\"caller_id\").head() # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 2 - use compute factory, control to `get JSON back` instead of PANDAS. \n",
    "#### `No saving to LAKE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_version_used = p.compute_factory.call_webservice(to_score,False,False) # (X_test, firstRowOnly=True,pandas_result=True, api_uri=None,api_key=\"auto from keyvault\")\n",
    "df_res = pd.read_json(result)\n",
    "to_score.join(df_res) # Need to join the FEATURES yourself, post webservice call (simulate no ESML dependancy in caller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alt 3 - Simulate client witn no ESML SDK, just using the \"scoring endpoint\". \n",
    "- Just JSON result (No ESML dependancy `get JSON back`)\n",
    "#### `No saving to LAKE` and `no JOIN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baselayer_azure_ml import ComputeFactory\n",
    "import json\n",
    "keyvault = p.ws.get_default_keyvault() # Authentica to your Azure ML workspace (ws)\n",
    "api_uri = keyvault.get_secret(name='esml-dev-p02-m10-api') \n",
    "api_key = keyvault.get_secret(name='esml-dev-p02-m10-apisecret') # DEV + Titanic\n",
    "\n",
    "#api_uri = keyvault.get_secret(name='esml-test-p02-m10-api') # TEST + Titanic\n",
    "#api_key = keyvault.get_secret(name='esml-test-p02-m10-apisecret')\n",
    "\n",
    "result_json = ComputeFactory.call_webservice_static(to_score, api_uri,api_key,firstRowOnly=False) # Simulate \"REST call\" (no ESML dependancy, just a wrapper for a pytnon REST call)\n",
    "res_dict = json.loads(result_json.text) # json -> dictionary\n",
    "df_res = pd.read_json(res_dict) # dictionary -> pandas\n",
    "all_result = X_test.join(df_res) # features + result\n",
    "all_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And....you need to save the data yourself to the lake, at this location\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\n",
    "print(\"Save your data here, if you want to have ADF WriteBack function\")\n",
    "print()\n",
    "print(scored_folder)\n",
    "print()\n",
    "print(\"Note: Last folder, UUID folder, should represent a 'unique scoring' for a day, but can be injected. Example: if we want a customerGUID instead \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA - more about `AutoLake Paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import repackage\n",
    "repackage.add(\"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLDataset, ESMLProject\n",
    "p = ESMLProject() \n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.inference_mode = True # This flag will \"change the paths\", from TRAIN folder to INFERENCE folder-structure\n",
    "\n",
    "print(\"\")\n",
    "print(\"INFERENCE\")\n",
    "print(\"\")\n",
    "\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    print(\"IN\", d.InPath)\n",
    "    print(\"Bronze\", d.BronzePath)\n",
    "    print(\"Silver\", d.SilverPath)\n",
    "\n",
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path(p.date_scoring_folder)\n",
    "print(\"Gold\", to_score_folder, \"  ...uuid folder, is to be able to have multiple unique scorings, same datetime\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TRAIN\")\n",
    "print(\"\")\n",
    "\n",
    "p.inference_mode = False # This flag will \"change the paths\"\n",
    "\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    print(\"IN\", d.InPath)\n",
    "    print(\"Bronze\", d.BronzePath)\n",
    "    print(\"Silver\", d.SilverPath)\n",
    "\n",
    "print(\"Gold\", p.GoldPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fec2c5a411dce07235ef28c8752b6cecf1f94423de7e7c24e62fc38b1bc47de"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azure_automl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
