{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCTION phase: About this notebook\n",
    "- Purpose: Creates 1 of the 2 PIPELINES\n",
    "    - `2a) training pipeline:` TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better.\n",
    "    \n",
    "## DETAILS - about this notebook and the 2a pipeline, generated\n",
    "- 1) Initiate ESMLPipelineFactory:\n",
    "- 2) `AUTO-GENERATE code: a snapshot folder` via ESML, that generates Python scripts and the `ESML runtime`\n",
    "    - 2_A_aml_pipeline\\4_inference\\batch\\\\`M11`\n",
    "        - Edit the feature engineerin files if needed\n",
    "            - 2_A_aml_pipeline\\4_inference\\batch\\\\`M11\\your_code\\your_custom_code.py`\n",
    "            - `your_custom_code.py` is referenced from all the `in_2_silver_...` files such as: 2_A_aml_pipeline\\4_inference\\batch\\M11\\\\`in2silver_ds01_diabetes.py` and `silver_merged_2_gold`\n",
    "        - Edit the AutoML train file, you need to add some configuration. See instructions in notebook cells below.\n",
    "            -2_A_aml_pipeline\\4_inference\\batch\\\\`M11\\train_post_automl_step.py`\n",
    "- 3) `BUILDS the pipeline` of certain (IN_2_GOLD_TRAIN_AUTOML)\n",
    "    - An `Azure machine learning pipeline` with steps will be automatically genereated, based on your `lake_settings.json` dataset array.\n",
    "    - It is a `training pipeline` of ESML type `IN_2_GOLD_TRAIN_AUTOML`\n",
    "- 4) `EXECUTES the pipeline` (smoke testing purpose - see that it works...)\n",
    "    - 4a) The below happens in the pipeline steps:Training pipeline: (`IN_2_GOLD_TRAIN_AUTOML`) steps:\n",
    "        - Feature engineering of each in-data - via `IN_2_SILVER` steps.\n",
    "        - Merges all SILVERS to `GOLD`\n",
    "        - Splits the `GOLD` to 3 buckets: `GOLD_TRAIN, GOLD_VALIDATE, GOLD_TEST`\n",
    "        - Trains model\n",
    "        - Registers the newly trained model, tags it as `newly_trained`\n",
    "        - Calculates test_set scoring with the `ESMLTestescoringFactory`\n",
    "        - `INNER LOOP MLOps:` Compares in current environment `DEV` if model should be promoted, based on `test_set_scoring`\n",
    "        - `OUTER LOOP MLOps:` Compares in next environment `TEST` if model should be promoted, based on `test_set_scoring` \n",
    "            - E.g. compares best model in `DEV` with the leading model in `TEST`\n",
    "- 5) PUBLISH the pipeline\n",
    "    - Purpose: Now when the pipeline is `smoke tested`, we can publish is, to get a `pipeline_id to use in Azure Data factory`\n",
    "    - We want to PRINT the `pipeline ID` after publish also, for easy access to use in `Azure data factory` for retraining on new data continously (DataOps & MLOps)\n",
    "- DONE.\n",
    "    \n",
    "\n",
    "Note:This notebook is called: `M11_v143_esml_regression_batch_train_automl.ipynb` in the notebook_templates folder\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for you: CONFIGURATION\n",
    "- 1) Change `p.active_model=11` to correct model number `1` if your model has that number.\n",
    "    - See  [lake_settings.json](./settings/project_specific/model/lake_settings.json) to find YOUR model number.\n",
    "- 2) After you run the cell [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder), you need to add YOUR feature engineering logic\n",
    "    -  This code you probably already have, from the R&D phase, in this CUSTOMIZE cell in the notebook: [1_R&D_phase_M10_M11.ipynb](./1_quickstart/1_R&D_phase_M10_M11.ipynb)\n",
    "        - You need to this code to the `your_custom_code.py` after you have genereated the snapshot folder, for it to be reachable and uploaded at pipeline creation.\n",
    "        - Tip: You can CREATE A CLASS, and add static methods, e.g. `ds01_process_in2silver(dataframe1)`  in the `your_custom_code.py` \n",
    "- 3) Now you have your code in the `your_custom_code.py`, then you need to reference that code from the auto-generated pipeline-steps files such as `in2silver_ds01_diabetes.py`\n",
    "    - Note: This snapshot folder will not exist, until you have run the first 2 cells in this notebook, or after this cell has run [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initiate ESMLPipelineFactory (Always run thic CELL below)\n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipelinbe to know the schema of data\n",
    "- To init the ESMLPipelinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n",
      "\n",
      " ---- Q: WHICH files are generated as templates, for you to EDIT? ---- \n",
      "A: These files & locations:\n",
      "File to EDIT (step: IN_2_SILVER_1): ../../../01_pipelines/M11/in2silver_ds01_diabetes.py\n",
      "File to EDIT (step: IN_2_SILVER_2): ../../../01_pipelines/M11/in2silver_ds02_other.py\n",
      "File to EDIT (step: SILVER_MERGED_2_GOLD): ../../../01_pipelines/M11/silver_merged_2_gold.py\n",
      "File to EDIT (step: SCORING_GOLD): ../../../01_pipelines/M11/scoring_gold.py\n",
      "File to EDIT (step: TRAIN_SPLIT_AND_REGISTER): ../../../01_pipelines/M11/train_split_and_register.py\n",
      "File to EDIT (step: TRAIN_MANUAL): ../../../01_pipelines/M11/train_manual.py\n",
      "File to EDIT (step: TRAIN_AUTOML): ../../../01_pipelines/M11/train_post_automl_step.py\n",
      "File to EDIT a lot (reference in step-scripts Custom code): ../../../01_pipelines/M11/your_code/your_custom_code.py\n",
      "\n",
      " ---- WHAT model to SCORE with, & WHAT data 'date_folder'? ---- \n",
      "InferenceModelVersion (model version to score with): 0\n",
      "Date_scoring_folder (data to score) : 1000-01-01 10:35:01.243860\n",
      "ESML environment: dev\n",
      "Inference mode (self.batch_pipeline_parameters[4]): 0\n",
      "\n",
      " ---- ESML Datalake locations: ESML Datasets (IN-data) ---- \n",
      "Name (lake folder): ds01_diabetes and AzureName IN: M11_ds01_diabetes_train_IN\n",
      "IN projects/project001/11_diabetes_model_reg/train/ds01_diabetes/in/dev/1000/01/01/\n",
      "Bronze projects/project001/11_diabetes_model_reg/train/ds01_diabetes/out/bronze/dev/\n",
      "Silver projects/project001/11_diabetes_model_reg/train/ds01_diabetes/out/silver/dev/\n",
      "\n",
      "Name (lake folder): ds02_other and AzureName IN: M11_ds02_other_train_IN\n",
      "IN projects/project001/11_diabetes_model_reg/train/ds02_other/in/dev/1000/01/01/\n",
      "Bronze projects/project001/11_diabetes_model_reg/train/ds02_other/out/bronze/dev/\n",
      "Silver projects/project001/11_diabetes_model_reg/train/ds02_other/out/silver/dev/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_in_folder.json',\n",
    "p.inference_mode = False\n",
    "p.active_model = 11 # 10=titanic , 11=Diabetes\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "training_datefolder = '1000-01-01 10:35:01.243860' # Will override active_in_folder.json\n",
    "p_factory.batch_pipeline_parameters[0].default_value = 0 # Will override active_in_folder.json.model.version = 0 meaning that ESML will find LATEST PROMOTED, and not use a specific Model.version. It will read data from .../inference/0/... folder\n",
    "p_factory.batch_pipeline_parameters[1].default_value = training_datefolder # overrides ESMLProject.date_scoring_folder.\n",
    "p_factory.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One time a day\" - the below is needed to be done, to ensure Azure ML v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"NB! The below command you only need to run 1 time a day - then you can disable this cell. comment the code lines\")\n",
    "print(\"\")\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells for an IN_2_GOLD_TRAIN_AUTOML pipeline will:\n",
    "- 1) Generate code files\n",
    "- 2) Build pipeline, ESML autoguild this, and will upload the snapshot folder together with the Azure ML pipeline.\n",
    "- 3) Run the pipeline. Smoke testing, see that it works\n",
    "- 4) IF it works, Publish the pipeline, or else, edit the code files or configuration, retry step 2 and 3.\n",
    "- 5) Print the pipeline_id, that is essential to use from Azure Data factory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `AUTO-GENERATE code: a snapshot folder`\n",
    "<a id='2_generate_snapshot_folder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n"
     ]
    }
   ],
   "source": [
    "## Generate CODE - then edit it to get correct environments\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING (3a,4a,5a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) `BUILDS the TRANING pipeline`\n",
    "- esml_pipeline_types.IN_2_GOLD_TRAIN_AUTOML\n",
    "- Take note on the `esml_pipeline_types` below, of type: esml_pipeline_types.`IN_2_GOLD_TRAIN_AUTOML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: ESML-AzureML-144-AutoML_126\n",
      "Active default environment IMAGE:  mcr.microsoft.com/azureml/curated/azureml-automl:126\n"
     ]
    }
   ],
   "source": [
    "print(\"Active environment:\", p_factory.environment_name)\n",
    "print(\"Active default environment IMAGE: \", p_factory._default_base_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and set new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New image:  mcr.microsoft.com/azureml/curated/azureml-automl:131\n",
      "New environment_name:  ESML-AzureML-144-AutoML_131\n",
      "Active environment: ESML-AzureML-144-AutoML_131\n"
     ]
    }
   ],
   "source": [
    "automl_version = \"131\" # Azure ML SDK 1.48 uses AutoML environembt with image v 131 # mcr.microsoft.com/azureml/curated/azureml-automl:131\n",
    "environment_name = \"ESML-AzureML-144-AutoML_\"+automl_version\n",
    "curr_name = p_factory._default_base_image\n",
    "base_image_in = curr_name.replace(curr_name[len(curr_name)- 3:], automl_version) # base_image_in = \"mcr.microsoft.com/azureml/curated/azureml-automl:128\"\n",
    "\n",
    "print(\"New image: \",base_image_in)\n",
    "print(\"New environment_name: \",environment_name)\n",
    "\n",
    "p.ws = p.get_workspace_from_config()\n",
    "#p_factory.create_automl_lts_environment(base_image_in, environment_name) # 1) Create a NEW AutoML environment\n",
    "p_factory.environment_name = environment_name # 2) Set this NEW automl environment\n",
    "\n",
    "print(\"Active environment:\", p_factory.environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set new environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: ESML-AzureML-144-AutoML_131\n"
     ]
    }
   ],
   "source": [
    "automl_version = \"131\" # Azure ML SDK 1.48 uses AutoML environembt with image v 131 # mcr.microsoft.com/azureml/curated/azureml-automl:131\n",
    "environment_name = \"ESML-AzureML-144-AutoML_\"+automl_version\n",
    "p_factory.environment_name = environment_name # 2) Set this NEW automl environment\n",
    "print(\"Active environment:\", p_factory.environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GEN2 as Datastore\n",
      "use_project_sp_2_mount: True\n",
      "Using Azure ML Environment: 'ESML-AzureML-144-AutoML_131' as primary environment for PythonScript Steps\n",
      "ESML will auto-create a compute...\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Using a model specific cluster, per configuration in project specific settings, (the integer of 'model_number' is the base for the name)\n",
      "Note: OVERRIDING enterprise performance settings with project specifics. (to change, set flag in 'dev_test_prod_settings.json' -> override_enterprise_settings_with_model_specific=False)\n",
      "Found existing cluster p001-m11weu-dev for project and environment, using it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "image_build_compute = p001-m11weu-dev\n",
      "Initiated DEFAULT compute - for DATASETS\n",
      "Reusing existing compute...\n",
      "Reusing existing compute...\n",
      "ESML regular mode\n",
      "datasets_list_with_dbx \n",
      "create_gold_train_step: inference_mode=False\n",
      "create_gold_train_step: inference_mode=False\n",
      "ESML-train_path_out = projects/project001/11_diabetes_model_reg/train/gold/dev/Train/{run-id}/\n",
      "Adding train step, creating...\n",
      "Loading AutoML config settings from: dev\n",
      "Searching with Model list LAMBDA FILTER, on experiment_name in Model.tags called: 11_diabetes_model_reg . Meaning ESML checks for both Notebook run (AutoMLRun, Run) and PipelineRuns (AutoMLStep, PipelineRun)\n",
      "E.g. Even if Pipeline experiment is called '11_diabetes_model_reg_IN_2_GOLD_TRAIN' it will be included, since original model_folder_name in ESML is '11_diabetes_model_reg' as a notebook Run experiment name. Both is included in search\n",
      "Filter search, minutes: 0.4215338706970215\n",
      "Current BEST model is: 11_diabetes_model_reg from Model registry with experiment_name-TAG 11_diabetes_model_reg, run_id-TAG d13a69bb-9c25-4d20-80e3-d9d1f9b8bfe9  model_name-TAG 11_diabetes_model_reg\n",
      "esml_time_updated: 01/18/2023, 01:48:34\n",
      "status_code : esml_promoted_2_dev\n",
      "model_name  : 11_diabetes_model_reg\n",
      "trained_in_workspace   : aml-prj001-weu-DEV-001\n",
      "current worksdpace p.ws  : aml-prj001-weu-DEV-001\n",
      "train_folder_template_with_date_id: projects/project001/11_diabetes_model_reg/train/gold/dev/{id_folder}/\n"
     ]
    }
   ],
   "source": [
    "## batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL, same_compute_for_all=True, cpu_gpu_databricks=\"cpu\", allow_reuse=True)\n",
    "\n",
    "## BUILD (takes ~10-12minutes)\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_AUTOML)\n",
    "# ...which Trains a model on data via date_folder parameters, upload the generated python scripts., and your custom code and ESML runtime, to Azure embedded in the pipeline, using Dockerized image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) `EXECUTES the pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB! Run in v1 legacy mode\n",
    "- You need to have your Azure Machine Learning workspace set to `v1_legacy_mode=True`\n",
    "- HOW do I know if I run v1 or v2? \n",
    "  - If you see this error message in `executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune`, containing the word in path `backendV2` when executing pipeline (cell below this), it is not in v1 legacy mode:\n",
    "     - <i>Failed to start the job for runid: 33ff1e3a-1ca7-4de0-bcee-b851cd2bb89d because of exception_type: ServiceInvocationException, error: Failure in StartSnapshotRun while calling service Execution; HttpMethod: POST; Response StatusCode: BadRequest; Exception type: Microsoft.RelInfra.Extensions.HttpRequestDetailException|-Microsoft.RelInfra.Common.Exceptions.ErrorResponseException, stack trace:    at Microsoft.Aether.EsCloud.Common.Client.ExecutionServiceClient.StartSnapshotRunAsync(String jobId, RunDefinition runDefinition, String runId, WorkspaceIdentity workspaceIdentity, String experimentName, CreatedBy createdBy) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\\\`backendV2`\\\\Clouds\\ESCloud\\ESCloudCommon\\Client\\ExecutionServiceClient.cs:line 162\n",
    "   at Microsoft.Aether.EsCloud.Common.JobProcessor.StartRunAsync(EsCloudJobMetadata job) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\backendV2\\Clouds\\ESCloud\\ESCloudCommon\\JobProcessor.cs:line 605\n",
    "   </i>\n",
    "- WHY? \n",
    "    - Azure ML SDK v2 does not yet (writing this 2022-10)support Spark jobs in pipeline, nor private endpoint.\n",
    "- TODO: To set the workspace in LEGACY v1 mode run this code 1 time, in a cell: `p.ws.update(v1_legacy_mode=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.ws = p.get_workspace_from_config()\n",
    "#p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute_pipeline (scoring): Inference_mode: 0\n",
      "-Scoring data, default value 1000-01-01 10:35:01.243860\n",
      "Adding pipeline parameters\n",
      "Created step IN 2 SILVER - ds01_diabetes [435dbc94][de841191-3525-406b-9074-3ed6c8708e6a], (This step will run and generate new outputs)\n",
      "Created step IN 2 SILVER - ds02_other [8460879e][98037144-8ce9-4e26-b634-1b1c691bebbc], (This step will run and generate new outputs)\n",
      "Created step SILVER MERGED 2 GOLD [3cb53c90][19af49d7-20c1-4ca4-9af9-60ebc0e827c4], (This step will run and generate new outputs)\n",
      "Created step SPLIT AND REGISTER datasets [2d951abb][c4e9519c-2acd-4c03-a21b-e40e6691e3e7], (This step will run and generate new outputs)\n",
      "Created step AutoML TRAIN in  [dev] [952d1c81][5f675dee-292d-4d1c-95e6-d7ac02d83c89], (This step will run and generate new outputs)\n",
      "Created step [dev]Calculate SCORING on TEST_SET, COMPARE & REGISTER model in [dev] & PROMOTE to [test] [b0647084][d40c667a-1f5e-4006-be09-c7f688527353], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 452f189d-1b5e-4352-be03-61e23cdda4ed\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/452f189d-1b5e-4352-be03-61e23cdda4ed?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n",
      "Pipeline submitted for execution!\n",
      " ### \n",
      "PipelineRunId: 452f189d-1b5e-4352-be03-61e23cdda4ed\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/452f189d-1b5e-4352-be03-61e23cdda4ed?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN and it will train in BIG Data, since using 100% Azure compute for all steps, including SPLITTING data\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline) # If this give ERROR message, looking at executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b) View meta data about the training run\n",
    "- What DATA was used, WHEN did the training occur, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mazureml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m----> 2\u001b[0m ds_name \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_GOLD_TRAINED_RUNINFO\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p\u001b[39m.\u001b[39mModelAlias)\n\u001b[0;32m      3\u001b[0m meta_ds\u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mget_by_name(workspace\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mws,name\u001b[39m=\u001b[39mds_name, version\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m meta_ds\u001b[39m.\u001b[39mto_pandas_dataframe()\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "ds_name =\"{}_GOLD_TRAINED_RUNINFO\".format(p.ModelAlias)\n",
    "meta_ds= Dataset.get_by_name(workspace=p.ws,name=ds_name, version='latest')\n",
    "meta_ds.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a) PUBLISH the TRAINING pipeline & PRINT its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT: Get info to use in Azure data factory\n",
    "- `published_pipeline.id` (if private Azure ML workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  291e51dd-0970-4570-a86d-765b843e5d6f\n",
      "Endpoint Name:  11_diabetes_model_reg_pipe_IN_2_GOLD_TRAIN_AUTOML_EP_1\n",
      "Experiment name:  11_diabetes_model_reg_pipe_IN_2_GOLD_TRAIN_AUTOML\n",
      "In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\n",
      "-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a6005129-079e-4c24-a757-90ecf76ddcb5'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))\n",
    "\n",
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! Next Step - Deploy model, serve your model for INFERENCING purpose:\n",
    "- For INFERENCE you may need either to DEPLOY the model \n",
    "    - a) ONLINE on AKS endpoint\n",
    "        - Notebook: \n",
    "    - b) BATCH SCORING on an Azure machine learning pipeline\n",
    "        - Notebook: [your_root]\\notebook_templates_quickstart\\\\`3a_PRODUCTION_phase_BATCH_INFERENCE_Pipeline.ipynb`\n",
    "    - c) STREAMING using Eventhubs and Azure Databricks structured streaming\n",
    "        - Notebook: TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: `Next step in PRODUCTION phaase after the 2a and 3a or 3b notebooks are done?`\n",
    " \n",
    "- 1) `DataOps+MLOps:` Go to your ESMLProjects `Azure data factory`, and use the `ESML DataOps templates` (Azure data factory templates) for `IN_2_GOLD_TRAIN` and `IN_2_GOLD_SCORING`\n",
    "    - azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\adf\\v1_3\\PROJECT000\\LakeOnly\\\\`STEP03_IN_2_GOLD_TRAIN_v1_3.zip`\n",
    "- 2) `MLOps CI/CD` Go to the next notebook `mlops` folder, to setup `CI/CD` in Azure Devops\n",
    "    - Import this in Azure devops\n",
    "        azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\mlops\\01_template_v14\\azure-devops-build-pipeline-to-import\\\\`ESML-v14-project002_M11-DevTest.json`\n",
    "    - Change the Azure Devops `VARIABLES` for service principle, tenant, etc.\n",
    "    - Change parameters in the `inlince Azure CLI script` to correct model you want to work with, and the correct data you want to train with, or score.\n",
    "        - Step `21-train_in_2_gold_train_pipeline`\n",
    "        - INLINE code calls the file: `21-train_in_2_gold_train_pipeline.py`\n",
    "        - INLINE parameters: `--esml_model_number 11 --esml_date_utc \"1000-01-01 10:35:01.243860\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA - DEBUG code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG - if Last pipeline step failed to register model:\n",
    "- How to register a model, from Pipeline with AutoMLStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.core import Model\n",
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "\n",
    "your_model_id = \"11_diabetes_model_reg\" # See Azure ML Studio - Models registry, 1st column in table\n",
    "automl_step_id = \"b5a41656-afe0-4e72-9e43-f45e39ca7d51\" # See in or in Pipeline outputs and logs, for last step Calculate..., or Azure ML Studio - Models registry,  2nd column in table. If empty, see JOBS id for run_id\n",
    "\n",
    "#model = Model(p.ws, your_model_id)\n",
    "run,best_run,fitted_model = IESMLController.init_run(p.ws,p.experiment_name, automl_step_id)\n",
    "\n",
    "model = best_run.parent.register_model(model_name=your_model_id, model_path=\"outputs\") # Works: 2023-01-12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_automl_esml_v148",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4799850f3103fb5d9644ce9433832c953591f6eac3309b593d58ecf6d126f819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
