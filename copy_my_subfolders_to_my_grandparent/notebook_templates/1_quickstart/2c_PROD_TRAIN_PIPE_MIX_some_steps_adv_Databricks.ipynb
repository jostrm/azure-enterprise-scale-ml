{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R&D or PRODUCTION phase: This will generate a PIPELINE with 1-M Databricks steps\n",
    "- All Databricks steps, or mixed with Python steps\n",
    "- Purpose: Creates 1 of the 2 PIPELINES\n",
    "    - `2a) training pipeline:` TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better. \n",
    "\n",
    "# Prerequisite - Databricks:\n",
    "- You need to have a `ESML Databricks template snaphshot folder` (M01,M11, ...) in your Databricks workspace\n",
    "- Run the notebooks in that folder first, interactively, so you know they work - then you come back to THIS notebooks, to generate an Azure ML pipeline, pointing at those Databricks notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for you: CONFIGURATION\n",
    "- 1) Change `p.active_model=11` to correct model number `1` if your model has that number.\n",
    "    - See  [lake_settings.json](./settings/project_specific/model/lake_settings.json) to find YOUR model number.\n",
    "- 2) After you run the cell [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder), you need to add YOUR feature engineering logic\n",
    "    -  This code you probably already have, from the R&D phase, in this CUSTOMIZE cell in the notebook: [1_R&D_phase_M10_M11.ipynb](./1_quickstart/1_R&D_phase_M10_M11.ipynb)\n",
    "        - 2a) You need to add this code to the `your_custom_code.py` after you have genereated the snapshot folder, for it to be reachable and uploaded at pipeline creation.\n",
    "            - Tip: You can CREATE A CLASS, and add static methods, e.g. `ds01_process_in2silver(dataframe1)`  in the `your_custom_code.py` \n",
    "        - 2b) You need to create the Databricks mapping here `ESMLPipelineStepMap.py`, define which steps are going to be handled by a Databricks notebooks and Databricks Sparl cluster.\n",
    "- 3) Now you have your code in the `your_custom_code.py`, then you need to reference that code from the auto-generated pipeline-steps files such as `in2silver_ds01_diabetes.py`\n",
    "    - Note: This snapshot folder will not exist, until you have run the first 2 cells in this notebook, or after this cell has run [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initiate ESMLPipelineFactory (Always run thic CELL below)\n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipelinbe to know the schema of data\n",
    "- To init the ESMLPipelinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_in_folder.json',\n",
    "p.inference_mode = False\n",
    "p.active_model = 11 # 10=titanic , 11=Diabetes\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "training_datefolder = '1000-01-01 10:35:01.243860' # Will override active_in_folder.json\n",
    "p_factory.batch_pipeline_parameters[0].default_value = 0 # Will override active_in_folder.json.model.version = 0 meaning that ESML will find LATEST PROMOTED, and not use a specific Model.version. It will read data from .../inference/0/... folder\n",
    "p_factory.batch_pipeline_parameters[1].default_value = training_datefolder # overrides ESMLProject.date_scoring_folder.\n",
    "\n",
    "all_steps_databricks = False #Notebook parameter: Disabled CELL that includes all mapped steps as DatabricksSteps\n",
    "simple_mode_but_separate_compute = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One time a day\" - the below is needed to be done, to ensure Azure ML v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NB! The below command you only need to run 1 time a day - then you can disable this cell. comment the code lines\")\n",
    "print(\"\")\n",
    "# Set LEGACY mode - Azure ML v1 - since private link and DatabricksStep\n",
    "p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells for an IN_2_GOLD_TRAIN_AUTOML pipeline will:\n",
    "- 1) Generate code files\n",
    "- 2) Build pipeline, ESML autoguild this, and will upload the snapshot folder together with the Azure ML pipeline.\n",
    "- 3) Run the pipeline. Smoke testing, see that it works\n",
    "- 4) IF it works, Publish the pipeline, or else, edit the code files or configuration, retry step 2 and 3.\n",
    "- 5) Print the pipeline_id, that is essential to use from Azure Data factory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `AUTO-GENERATE code: a snapshot folder`\n",
    "<a id='2_generate_snapshot_folder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate CODE - then edit it to get correct environments\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative A) - Filter out, use some steps, a whitelist\n",
    "- Using a whitelist filter. 1-M of your mapped steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import esml_snapshot_step_names\n",
    "sys.path.insert(0, \"../01_pipelines/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "dataset_folder_names = p.active_model['dataset_folder_names']\n",
    "step1 = esml_snapshot_step_names.in2silver_template.format(dataset_folder_names[0])\n",
    "step2 = esml_snapshot_step_names.in2silver_template.format(dataset_folder_names[1])\n",
    "step3 = esml_snapshot_step_names.silver_merged_2_gold\n",
    "step4 = esml_snapshot_step_names.train_split_and_register\n",
    "step5 = esml_snapshot_step_names.train_manual\n",
    "\n",
    "step_filter_whitelist = [step1,step2,step3]\n",
    "\n",
    "my_map = ESMLPipelineStepMap(step_filter_whitelist) # TODO 4 YOU: You need to implement this class. See \"your_code\" folder \n",
    "#map = ESMLPipelineStepMap()\n",
    "p_factory.use_advanced_compute_settings(my_map)\n",
    "\n",
    "# Print the Mappning\n",
    "train_map = my_map.get_train_map(p.active_model['dataset_folder_names']) # Get the map\n",
    "train_map # prints it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative B) - Use all possible steps you defined in the ESMLPipleineStepMap\n",
    "- No whitelist filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "sys.path.insert(0, \"../01_pipelines/batch/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "if(all_steps_databricks):\n",
    "    mapping = ESMLPipelineStepMap() # TODO 4 YOU: You need to implement this class. See \"your_code\" folder \n",
    "    p_factory.use_advanced_compute_settings(mapping)\n",
    "\n",
    "    # Print the Mappning\n",
    "    train_map = mapping.get_train_map(p.active_model['dataset_folder_names']) # Get the map\n",
    "    train_map # prints it\n",
    "else:\n",
    "    print(\"This notebook CELL is disabled. Change 'all_steps_databricks=True' to enable it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View pipeline steps, and its types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in p_factory.pipeline_steps_array:\n",
    "    print(type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING (3a,4a,5a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) `BUILDS the TRANING pipeline`\n",
    "- esml_pipeline_types.IN_2_GOLD_TRAIN_AUTOML\n",
    "- Take note on the `esml_pipeline_types` below, of type: esml_pipeline_types.`IN_2_GOLD_TRAIN_AUTOML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUILD (takes ~6-12minutes)\n",
    "if(simple_mode_but_separate_compute):\n",
    "    p_factory.use_advanced_compute_settings(None)\n",
    "    batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL, same_compute_for_all=False, aml_compute=None, allow_reuse=True)\n",
    "else:\n",
    "    batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL)\n",
    "    #batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL, same_compute_for_all=True, aml_compute=None, allow_reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) `EXECUTES the pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB! Run in v1 legacy mode\n",
    "- You need to have your Azure Machine Learning workspace set to `v1_legacy_mode=True`\n",
    "- HOW do I know if I run v1 or v2? \n",
    "  - If you see this error message in `executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune`, containing the word in path `backendV2` when executing pipeline (cell below this), it is not in v1 legacy mode:\n",
    "     - <i>Failed to start the job for runid: 33ff1e3a-1ca7-4de0-bcee-b851cd2bb89d because of exception_type: ServiceInvocationException, error: Failure in StartSnapshotRun while calling service Execution; HttpMethod: POST; Response StatusCode: BadRequest; Exception type: Microsoft.RelInfra.Extensions.HttpRequestDetailException|-Microsoft.RelInfra.Common.Exceptions.ErrorResponseException, stack trace:    at Microsoft.Aether.EsCloud.Common.Client.ExecutionServiceClient.StartSnapshotRunAsync(String jobId, RunDefinition runDefinition, String runId, WorkspaceIdentity workspaceIdentity, String experimentName, CreatedBy createdBy) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\\\`backendV2`\\\\Clouds\\ESCloud\\ESCloudCommon\\Client\\ExecutionServiceClient.cs:line 162\n",
    "   at Microsoft.Aether.EsCloud.Common.JobProcessor.StartRunAsync(EsCloudJobMetadata job) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\backendV2\\Clouds\\ESCloud\\ESCloudCommon\\JobProcessor.cs:line 605\n",
    "   </i>\n",
    "- WHY? \n",
    "    - Azure ML SDK v2 does not yet (writing this 2022-10)support Spark jobs in pipeline, nor private endpoint.\n",
    "- TODO: To set the workspace in LEGACY v1 mode run this code 1 time, in a cell: `p.ws.update(v1_legacy_mode=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label = 'Y'\n",
    "train_df = aml.to_pandas_dataframe()\n",
    "#y1 = train_df[label]\n",
    "X=train_df.drop(label, axis=1)\n",
    "y = train_df.pop(label).to_frame()\n",
    "\n",
    "#print(y1.head()) # no column\n",
    "#print(type(y1)) # series\n",
    "\n",
    "print(X.head())\n",
    "print(\"\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN and it will train in BIG Data, since using 100% Azure compute for all steps, including SPLITTING data\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline) # If this give ERROR message, looking at executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b) View meta data about the training run\n",
    "- What DATA was used, WHEN did the training occur, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "ds_name =\"{}_GOLD_TRAINED_RUNINFO\".format(p.ModelAlias)\n",
    "meta_ds= Dataset.get_by_name(workspace=p.ws,name=ds_name, version='latest')\n",
    "meta_ds.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a) PUBLISH the TRAINING pipeline & PRINT its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT: Get info to use in Azure data factory\n",
    "- `published_pipeline.id` (if private Azure ML workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))\n",
    "\n",
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! Next Step - Deploy model, serve your model for INFERENCING purpose:\n",
    "- For INFERENCE you may need either to DEPLOY the model \n",
    "    - a) ONLINE on AKS endpoint\n",
    "        - Notebook: \n",
    "    - b) BATCH SCORING on an Azure machine learning pipeline\n",
    "        - Notebook: [your_root]\\notebook_templates_quickstart\\\\`3a_PRODUCTION_phase_BATCH_INFERENCE_Pipeline.ipynb`\n",
    "    - c) STREAMING using Eventhubs and Azure Databricks structured streaming\n",
    "        - Notebook: TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: `Next step in PRODUCTION phaase after the 2a and 3a or 3b notebooks are done?`\n",
    " \n",
    "- 1) `DataOps+MLOps:` Go to your ESMLProjects `Azure data factory`, and use the `ESML DataOps templates` (Azure data factory templates) for `IN_2_GOLD_TRAIN` and `IN_2_GOLD_SCORING`\n",
    "    - azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\adf\\v1_3\\PROJECT000\\LakeOnly\\\\`STEP03_IN_2_GOLD_TRAIN_v1_3.zip`\n",
    "- 2) `MLOps CI/CD` Go to the next notebook `mlops` folder, to setup `CI/CD` in Azure Devops\n",
    "    - Import this in Azure devops\n",
    "        azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\mlops\\01_template_v14\\azure-devops-build-pipeline-to-import\\\\`ESML-v14-project002_M11-DevTest.json`\n",
    "    - Change the Azure Devops `VARIABLES` for service principle, tenant, etc.\n",
    "    - Change parameters in the `inlince Azure CLI script` to correct model you want to work with, and the correct data you want to train with, or score.\n",
    "        - Step `21-train_in_2_gold_train_pipeline`\n",
    "        - INLINE code calls the file: `21-train_in_2_gold_train_pipeline.py`\n",
    "        - INLINE parameters: `--esml_model_number 11 --esml_date_utc \"1000-01-01 10:35:01.243860\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StepMap - how to print & look at it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_map = map.get_train_map(p.active_model['dataset_folder_names'])\n",
    "has_dbx,step_name,map_step = map.get_dbx_map_step(train_map,'ds01_diabetes')\n",
    "print(has_dbx)\n",
    "print(step_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_map = map.get_train_map(p.active_model['dataset_folder_names'])\n",
    "for d in p.Datasets:\n",
    "    print(d.Name)\n",
    "    has_dbx,step_name,map_step = map.get_dbx_map_step(train_map,d.Name)\n",
    "    print(\"has_dbx:\",has_dbx)\n",
    "    print(\"step_name\",step_name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('py38_default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b89d13efa15026bec8ac80f9cabf9db8b4aa027bf15218d19e9326d2a712ef0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
