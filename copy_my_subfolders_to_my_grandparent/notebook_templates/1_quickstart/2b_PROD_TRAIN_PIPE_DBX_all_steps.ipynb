{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCTION phase: About this notebook\n",
    "- Purpose: Creates 1 of the 2 PIPELINES\n",
    "    - `2a) training pipeline:` TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better.\n",
    "    \n",
    "## DETAILS - about this notebook and the 2a pipeline, generated\n",
    "- 1) Initiate ESMLPipelineFactory:\n",
    "- 2) `AUTO-GENERATE code: a snapshot folder` via ESML, that generates Python scripts and the `ESML runtime`\n",
    "    - 2_A_aml_pipeline\\4_inference\\batch\\\\`M11`\n",
    "        - Edit the feature engineerin files if needed\n",
    "            - 2_A_aml_pipeline\\4_inference\\batch\\\\`M11\\your_code\\your_custom_code.py`\n",
    "            - `your_custom_code.py` is referenced from all the `in_2_silver_...` files such as: 2_A_aml_pipeline\\4_inference\\batch\\M11\\\\`in2silver_ds01_diabetes.py` and `silver_merged_2_gold`\n",
    "        - Edit the AutoML train file, you need to add some configuration. See instructions in notebook cells below.\n",
    "            -2_A_aml_pipeline\\4_inference\\batch\\\\`M11\\train_post_automl_step.py`\n",
    "- 3) `BUILDS the pipeline` of certain (IN_2_GOLD_TRAIN_AUTOML)\n",
    "    - An `Azure machine learning pipeline` with steps will be automatically genereated, based on your `lake_settings.json` dataset array.\n",
    "    - It is a `training pipeline` of ESML type `IN_2_GOLD_TRAIN_AUTOML`\n",
    "- 4) `EXECUTES the pipeline` (smoke testing purpose - see that it works...)\n",
    "    - 4a) The below happens in the pipeline steps:Training pipeline: (`IN_2_GOLD_TRAIN_AUTOML`) steps:\n",
    "        - Feature engineering of each in-data - via `IN_2_SILVER` steps.\n",
    "        - Merges all SILVERS to `GOLD`\n",
    "        - Splits the `GOLD` to 3 buckets: `GOLD_TRAIN, GOLD_VALIDATE, GOLD_TEST`\n",
    "        - Trains model\n",
    "        - Registers the newly trained model, tags it as `newly_trained`\n",
    "        - Calculates test_set scoring with the `ESMLTestescoringFactory`\n",
    "        - `INNER LOOP MLOps:` Compares in current environment `DEV` if model should be promoted, based on `test_set_scoring`\n",
    "        - `OUTER LOOP MLOps:` Compares in next environment `TEST` if model should be promoted, based on `test_set_scoring` \n",
    "            - E.g. compares best model in `DEV` with the leading model in `TEST`\n",
    "- 5) PUBLISH the pipeline\n",
    "    - Purpose: Now when the pipeline is `smoke tested`, we can publish is, to get a `pipeline_id to use in Azure Data factory`\n",
    "    - We want to PRINT the `pipeline ID` after publish also, for easy access to use in `Azure data factory` for retraining on new data continously (DataOps & MLOps)\n",
    "- DONE.\n",
    "    \n",
    "\n",
    "Note:This notebook is called: `M11_v143_esml_regression_batch_train_automl.ipynb` in the notebook_templates folder\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for you: CONFIGURATION\n",
    "- 1) Change `p.active_model=11` to correct model number `1` if your model has that number.\n",
    "    - See  [lake_settings.json](./settings/project_specific/model/lake_settings.json) to find YOUR model number.\n",
    "- 2) After you run the cell [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder), you need to add YOUR feature engineering logic\n",
    "    -  This code you probably already have, from the R&D phase, in this CUSTOMIZE cell in the notebook: [1_R&D_phase_M10_M11.ipynb](./1_quickstart/1_R&D_phase_M10_M11.ipynb)\n",
    "        - You need to this code to the `your_custom_code.py` after you have genereated the snapshot folder, for it to be reachable and uploaded at pipeline creation.\n",
    "        - Tip: You can CREATE A CLASS, and add static methods, e.g. `ds01_process_in2silver(dataframe1)`  in the `your_custom_code.py` \n",
    "- 3) Now you have your code in the `your_custom_code.py`, then you need to reference that code from the auto-generated pipeline-steps files such as `in2silver_ds01_diabetes.py`\n",
    "    - Note: This snapshot folder will not exist, until you have run the first 2 cells in this notebook, or after this cell has run [2) AUTO-GENERATE code: a snapshot folder](#2_generate_snapshot_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initiate ESMLPipelineFactory (Always run thic CELL below)\n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipelinbe to know the schema of data\n",
    "- To init the ESMLPipelinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_in_folder.json',\n",
    "p.inference_mode = False\n",
    "p.active_model = 11 # 10=titanic , 11=Diabetes\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "training_datefolder = '1000-01-01 10:35:01.243860' # Will override active_in_folder.json\n",
    "p_factory.batch_pipeline_parameters[0].default_value = 0 # Will override active_in_folder.json.model.version = 0 meaning that ESML will find LATEST PROMOTED, and not use a specific Model.version. It will read data from .../inference/0/... folder\n",
    "p_factory.batch_pipeline_parameters[1].default_value = training_datefolder # overrides ESMLProject.date_scoring_folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One time a day\" - the below is needed to be done, to ensure Azure ML v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"NB! The below command you only need to run 1 time a day - then you can disable this cell. comment the code lines\")\n",
    "print(\"\")\n",
    "# Set LEGACY mode - Azure ML v1 - since private link and DatabricksStep\n",
    "p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below cells for an IN_2_GOLD_TRAIN_AUTOML pipeline will:\n",
    "- 1) Generate code files\n",
    "- 2) Build pipeline, ESML autoguild this, and will upload the snapshot folder together with the Azure ML pipeline.\n",
    "- 3) Run the pipeline. Smoke testing, see that it works\n",
    "- 4) IF it works, Publish the pipeline, or else, edit the code files or configuration, retry step 2 and 3.\n",
    "- 5) Print the pipeline_id, that is essential to use from Azure Data factory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `AUTO-GENERATE code: a snapshot folder`\n",
    "<a id='2_generate_snapshot_folder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n"
     ]
    }
   ],
   "source": [
    "## Generate CODE - then edit it to get correct environments\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks TODO 4 YOU:\n",
    "## 1) In Databricks: Create Databricks Access token, add to keyvault as secret `esml-project-dbx-token`\n",
    "- Visit the project specific Azure keyvault, `kv-p001-...` and create a new secret called `esml-project-dbx-token`, open a new TAB in your web browser\n",
    "- In Databricks: Click on `email/Use Settings/Generate new token`\n",
    "    - Comment optional, example: `azure ml pipeline`\n",
    "    - Life time, has this empty, since we will set expiration in Azure keuvault instead, for 2 years.\n",
    "    - Go to your other open web browser tab, paste to Azure keyvalt secret value box. Note: Secret should start with `dapi...`\n",
    "\n",
    "- VERIFY / TEST access to TOKEN like this: \n",
    "    ```python\n",
    "       p.ws = p.get_workspace_from_config()\n",
    "       dbx_token = p.ws.get_default_keyvault().get_secret(name='esml-project-dbx-token')\n",
    "    ```\n",
    "\n",
    "### 2) In Databricks: Make sure you have the notebooks, M11\n",
    "- Connect to REPO: The Azure devops repo. The M11 snapshot folder should be here\n",
    "    - notebooks_databricks/esml/...\n",
    "\n",
    "### 3) Create a FOLDER in the lake - if you are creating a manual ML model\n",
    "- Create a folder called 'model', under train, to keep your pickle files\n",
    "    - Example: ...11_diabetes_model_reg/train/model/\n",
    "\n",
    "### 4) Configure the ESMLPipelineStepMap\n",
    "Location is under your SNAPSHOT folder, after you generated files via `p_factory.create_dataset_scripts_from_template(overwrite_if_exists=True)`\n",
    "\n",
    "Location: `01_pipelines\\batch\\M11\\your_code\\ESMLPipelineStepMap.py`\n",
    "\n",
    "    - 3a) Set your dev,test,prod values:\n",
    "\n",
    "```python\n",
    "        all_envs = {\n",
    "        'dev': {'compute_name': None,'resource_group': 'abc-def-esml-project002-weu-dev-004-rg', 'workspace_name': 'z', 'access_token': 't'},\n",
    "        'test': {'compute_name': None,'resource_group': 'abc-def-esml-project002-weu-test-004-rg', 'workspace_name': 'z', 'access_token': 't'},\n",
    "        'prod': {'compute_name': None,'resource_group': 'abc-def-esml-project002-weu-prod-004-rg', 'workspace_name': 'z', 'access_token': 't'}\n",
    "        }\n",
    "```\n",
    "\n",
    "    - 3b) Configure the map, by implementing the method, under `01_pipelines\\batch\\M11\\your_code\\ESMLPipelineStepMap.py`\n",
    "\n",
    "```python\n",
    "         def your_train_map(self, dataset_folder_names):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Before running CELL below: You need to RESTART notebook - if you changed code in the `ESMLPipelineStepMap`\n",
    "- Before running Notebook again, remember to set `False`, for your config-code not to be overwritten, at cell above:\n",
    "```python\n",
    "    p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML Workspace:\n",
      "Attached Databricks db_compute_name:\n",
      "Compute target n1-p000-aml-91 already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'step_name': 'in2silver_ds01_diabetes',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/10_in2silver_ds01_diabetes',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds01_diabetes',\n",
       "  'dataset_filename_ending': '*.csv',\n",
       "  'compute_name': 'n1-p000-aml-91',\n",
       "  'cluster_id': '0111-230838-10wcl6d4'},\n",
       " {'step_name': 'in2silver_ds02_other',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/10_in2silver_ds02_other',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds02_other',\n",
       "  'dataset_filename_ending': '*.csv',\n",
       "  'compute_name': 'n1-p000-aml-91',\n",
       "  'cluster_id': '0111-230838-10wcl6d4'},\n",
       " {'step_name': 'silver_merged_2_gold',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/20_merge_2_gold',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': 'ds01_diabetes,ds02_other',\n",
       "  'dataset_filename_ending': '*.parquet',\n",
       "  'compute_name': 'n1-p000-aml-91',\n",
       "  'cluster_id': '0111-230838-10wcl6d4'},\n",
       " {'step_name': 'train_split_and_register',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/21_split_GOLD_and_register_datasets',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': '',\n",
       "  'dataset_filename_ending': '*.parquet',\n",
       "  'compute_name': 'n1-p000-aml-91',\n",
       "  'cluster_id': '0111-230838-10wcl6d4'},\n",
       " {'step_name': 'train_manual',\n",
       "  'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/30_train_register',\n",
       "  'compute_type': 'dbx',\n",
       "  'date_folder_or': None,\n",
       "  'dataset_folder_names': '',\n",
       "  'dataset_filename_ending': '*.parquet',\n",
       "  'compute_name': 'n1-p000-aml-91',\n",
       "  'cluster_id': '0111-230838-10wcl6d4'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "sys.path.insert(0, \"../01_pipelines/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "map = ESMLPipelineStepMap() # TODO 4 YOU: You need to implement this class. See \"your_code\" folder\n",
    "p_factory.use_advanced_compute_settings(map)\n",
    "\n",
    "train_map = map.get_train_map(p.active_model['dataset_folder_names'])\n",
    "train_map # prints  the map\n",
    "#has_dbx_silver_merged_2_gold_step,step_name,map_step = map.get_dbx_map_step(train_map,\"silver_merged_2_gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING (3a,4a,5a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) `BUILDS the TRANING pipeline`\n",
    "- esml_pipeline_types.IN_2_GOLD_TRAIN_AUTOML\n",
    "- Take note on the `esml_pipeline_types` below, of type: esml_pipeline_types.`IN_2_GOLD_TRAIN_AUTOML`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GEN2 as Datastore\n",
      "use_project_sp_2_mount: True\n",
      "Environment ESML-AzureML-144-AutoML_126 exists\n",
      "Using Azure ML Environment: 'ESML-AzureML-144-AutoML_126' as primary environment for PythonScript Steps\n",
      "Dataset: ds01_diabetes has advanced mapping - an Azure Databricks mapping\n",
      "Dataset: ds02_other has advanced mapping - an Azure Databricks mapping\n",
      "ESML advanced mode: with advanced compute mappings\n",
      " - Step: silver_merged_2_gold has advanced mapping - an Azure Databricks mapping\n",
      "Found attached Databricks compute cluster\n",
      "previous_step_is_databricks = 1\n",
      "create_gold_train_step: inference_mode=False\n",
      "par_date_utc: 1000-01-01 10:35:01.243860\n",
      "Created Databricks step in pipeline\n",
      " - Step: train_split_and_register = train_split_and_register has advanced mapping - an Azure Databricks mapping\n",
      "previous_step_is_databricks = 1\n",
      "INPUT GOLD (p.GoldPathDatabricks) is: projects/project001/11_diabetes_model_reg/train/gold/dev/gold_dbx.parquet/*.parquet\n",
      "ESML-train_path_out = projects/project001/11_diabetes_model_reg/train/gold/dev/Train/gold_train_dbx.parquet/\n",
      "Adding train step, creating...\n",
      " - Step: train_manual has advanced mapping - an Azure Databricks mapping for: IN_2_GOLD_TRAIN_MANUAL\n",
      "Searching with Model list LAMBDA FILTER, on experiment_name in Model.tags called: 11_diabetes_model_reg . Meaning ESML checks for both Notebook run (AutoMLRun, Run) and PipelineRuns (AutoMLStep, PipelineRun)\n",
      "E.g. Even if Pipeline experiment is called '11_diabetes_model_reg_IN_2_GOLD_TRAIN' it will be included, since original model_folder_name in ESML is '11_diabetes_model_reg' as a notebook Run experiment name. Both is included in search\n",
      "Filter search, minutes: 0.13251123428344727\n",
      "Current BEST model is: 11_diabetes_model_reg from Model registry with experiment_name-TAG 11_diabetes_model_reg, run_id-TAG AutoML_03534c53-e646-4372-8132-75e45d2fcaba  model_name-TAG 11_diabetes_model_reg\n",
      "esml_time_updated: 2023-01-12 01:22:13.990842\n",
      "status_code : esml_promoted_2_dev\n",
      "model_name  : 11_diabetes_model_reg\n",
      "trained_in_workspace   : aml-prj001-weu-DEV-001\n",
      "current worksdpace p.ws  : aml-prj001-weu-DEV-001\n",
      "train_folder_template_with_date_id: projects/project001/11_diabetes_model_reg/train/gold/dev/\n"
     ]
    }
   ],
   "source": [
    "## BUILD (takes ~10-12minutes)\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_TRAIN_MANUAL)\n",
    "# ...which Trains a model on data via date_folder parameters, upload the generated python scripts., and your custom code and ESML runtime, to Azure embedded in the pipeline, using Dockerized image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a) `EXECUTES the pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB! Run in v1 legacy mode\n",
    "- You need to have your Azure Machine Learning workspace set to `v1_legacy_mode=True`\n",
    "- HOW do I know if I run v1 or v2? \n",
    "  - If you see this error message in `executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune`, containing the word in path `backendV2` when executing pipeline (cell below this), it is not in v1 legacy mode:\n",
    "     - <i>Failed to start the job for runid: 33ff1e3a-1ca7-4de0-bcee-b851cd2bb89d because of exception_type: ServiceInvocationException, error: Failure in StartSnapshotRun while calling service Execution; HttpMethod: POST; Response StatusCode: BadRequest; Exception type: Microsoft.RelInfra.Extensions.HttpRequestDetailException|-Microsoft.RelInfra.Common.Exceptions.ErrorResponseException, stack trace:    at Microsoft.Aether.EsCloud.Common.Client.ExecutionServiceClient.StartSnapshotRunAsync(String jobId, RunDefinition runDefinition, String runId, WorkspaceIdentity workspaceIdentity, String experimentName, CreatedBy createdBy) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\\\`backendV2`\\\\Clouds\\ESCloud\\ESCloudCommon\\Client\\ExecutionServiceClient.cs:line 162\n",
    "   at Microsoft.Aether.EsCloud.Common.JobProcessor.StartRunAsync(EsCloudJobMetadata job) in D:\\a\\_work\\1\\s\\src\\aether\\platform\\backendV2\\Clouds\\ESCloud\\ESCloudCommon\\JobProcessor.cs:line 605\n",
    "   </i>\n",
    "- WHY? \n",
    "    - Azure ML SDK v2 does not yet (writing this 2022-10)support Spark jobs in pipeline, nor private endpoint.\n",
    "- TODO: To set the workspace in LEGACY v1 mode run this code 1 time, in a cell: `p.ws.update(v1_legacy_mode=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p.ws = p.get_workspace_from_config()\n",
    "#p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute_pipeline (scoring): Inference_mode: 0\n",
      "-Scoring data, default value 1000-01-01 10:35:01.243860\n",
      "Created step in2silver_ds01_diabetes [db16530b][fd86ad54-6397-4e46-bcb8-a775718be57c], (This step will run and generate new outputs)\n",
      "Created step in2silver_ds02_other [7c0f8c6a][ff1d2f94-71ce-40da-a4c4-5a4f642024e7], (This step will run and generate new outputs)\n",
      "Created step silver_merged_2_gold [a2239267][af83568c-a2ac-4426-b989-1b88e4993b83], (This step will run and generate new outputs)\n",
      "Created step SPLIT AND REGISTER (0.6 % TRAIN) [d2244f74][7d277cf6-9e86-4575-aef4-50b98b17abcd], (This step will run and generate new outputs)\n",
      "Created step TRAIN in  [dev], COMPARE & REGISTER model in [dev] & PROMOTE to [test] [66339d7c][ee010795-c88a-4604-a2e1-42e3f65e9636], (This step will run and generate new outputs)\n",
      "Created data reference M11_ds01_diabetes_train_IN for StepId [abac3939][5ab5f509-18b9-4dfe-8bb7-661181fd46a3], (Consumers of this data will generate new runs.)Created data reference M11_ds02_other_train_IN for StepId [077417d8][5c263e62-6a15-42ef-97da-23d35e74e81e], (Consumers of this data will generate new runs.)\n",
      "\n",
      "Submitted PipelineRun 6fd86ddc-beb4-4ef6-94ad-dc71875c386f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6fd86ddc-beb4-4ef6-94ad-dc71875c386f?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n",
      "Pipeline submitted for execution!\n",
      " ### \n",
      "PipelineRunId: 6fd86ddc-beb4-4ef6-94ad-dc71875c386f\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/6fd86ddc-beb4-4ef6-94ad-dc71875c386f?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN and it will train in BIG Data, since using 100% Azure compute for all steps, including SPLITTING data\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline) # If this give ERROR message, looking at executionlogs.txt in Azure machine learning studio Output+logs tab on pipeline rune\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b) View meta data about the training run\n",
    "- What DATA was used, WHEN did the training occur, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_run_id</th>\n",
       "      <th>scored_gold_path</th>\n",
       "      <th>date_in_parameter</th>\n",
       "      <th>date_at_pipeline_run</th>\n",
       "      <th>model_version</th>\n",
       "      <th>used_model_version</th>\n",
       "      <th>used_model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34cdc542-6b8e-454d-afa5-47fb243e48e6</td>\n",
       "      <td>projects/project001/11_diabetes_model_reg/inference/0/scored/dev/1000/01/01/34cdc542-6b8e-454d-afa5-47fb243e48e6/</td>\n",
       "      <td>1000-01-01 10:35:01.243860</td>\n",
       "      <td>2023-01-13 15:29:47.611164</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11_diabetes_model_reg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        pipeline_run_id  \\\n",
       "0  34cdc542-6b8e-454d-afa5-47fb243e48e6   \n",
       "\n",
       "                                                                                                    scored_gold_path  \\\n",
       "0  projects/project001/11_diabetes_model_reg/inference/0/scored/dev/1000/01/01/34cdc542-6b8e-454d-afa5-47fb243e48e6/   \n",
       "\n",
       "            date_in_parameter        date_at_pipeline_run model_version  \\\n",
       "0  1000-01-01 10:35:01.243860  2023-01-13 15:29:47.611164             0   \n",
       "\n",
       "  used_model_version        used_model_name  \n",
       "0                  2  11_diabetes_model_reg  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds_name =\"{}_GOLD_SCORED_RUNINFO\".format(p.ModelAlias)\n",
    "meta_ds= Dataset.get_by_name(workspace=p.ws,name=ds_name, version='latest')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "meta_ds.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5a) PUBLISH the TRAINING pipeline & PRINT its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT: Get info to use in Azure data factory\n",
    "- `published_pipeline.id` (if private Azure ML workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  44be26e4-f92a-4f91-a028-56d1cf64be39\n",
      "Endpoint Name:  11_diabetes_model_reg_pipe_IN_2_GOLD_TRAIN_EP_3_dbx\n",
      "Experiment name:  11_diabetes_model_reg_pipe_IN_2_GOLD_TRAIN\n",
      "In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\n",
      "-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ecb206b1-59b7-4d53-8d82-a97811445566'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))\n",
    "\n",
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE! Next Step - Deploy model, serve your model for INFERENCING purpose:\n",
    "- For INFERENCE you may need either to DEPLOY the model \n",
    "    - a) ONLINE on AKS endpoint\n",
    "        - Notebook: \n",
    "    - b) BATCH SCORING on an Azure machine learning pipeline\n",
    "        - Notebook: [your_root]\\notebook_templates_quickstart\\\\`3a_PRODUCTION_phase_BATCH_INFERENCE_Pipeline.ipynb`\n",
    "    - c) STREAMING using Eventhubs and Azure Databricks structured streaming\n",
    "        - Notebook: TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: `Next step in PRODUCTION phaase after the 2a and 3a or 3b notebooks are done?`\n",
    " \n",
    "- 1) `DataOps+MLOps:` Go to your ESMLProjects `Azure data factory`, and use the `ESML DataOps templates` (Azure data factory templates) for `IN_2_GOLD_TRAIN` and `IN_2_GOLD_SCORING`\n",
    "    - azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\adf\\v1_3\\PROJECT000\\LakeOnly\\\\`STEP03_IN_2_GOLD_TRAIN_v1_3.zip`\n",
    "- 2) `MLOps CI/CD` Go to the next notebook `mlops` folder, to setup `CI/CD` in Azure Devops\n",
    "    - Import this in Azure devops\n",
    "        azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\mlops\\01_template_v14\\azure-devops-build-pipeline-to-import\\\\`ESML-v14-project002_M11-DevTest.json`\n",
    "    - Change the Azure Devops `VARIABLES` for service principle, tenant, etc.\n",
    "    - Change parameters in the `inlince Azure CLI script` to correct model you want to work with, and the correct data you want to train with, or score.\n",
    "        - Step `21-train_in_2_gold_train_pipeline`\n",
    "        - INLINE code calls the file: `21-train_in_2_gold_train_pipeline.py`\n",
    "        - INLINE parameters: `--esml_model_number 11 --esml_date_utc \"1000-01-01 10:35:01.243860\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_automl_esml_v148",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "4799850f3103fb5d9644ce9433832c953591f6eac3309b593d58ecf6d126f819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
