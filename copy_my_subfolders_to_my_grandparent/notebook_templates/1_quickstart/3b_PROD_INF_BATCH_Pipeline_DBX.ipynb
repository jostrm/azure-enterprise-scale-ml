{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initiate ESMLPipelineFactory (Always run thic CELL below)\n",
    "- To attach ESML controlplane to your project\n",
    "- To point at `template-data` for the pipeline to know the schema of data.\n",
    "    - NB! Azure machine learning pipelines need sample data. You need to have sample-data underneath the datalake folder structure:\n",
    "    - `0` is recommended for `model_version folder`\n",
    "        - Note: Ensure there is data under that version folder:  project002/11_diabetes_model_reg/inference/`0`/ds01_diabetes/in/dev/`1000/01/01/`\n",
    "    - `1000-01-01 00:00:00.243860` is recommended for `date_folder`\n",
    "        - Example: project002/11_diabetes_model_reg/inference/`1`/ds01_diabetes/in/dev/`1000/01/01/`\n",
    "- To init the ESMLPipelinefactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lake_settings.json with ESML version 1.4 - Models array support including LABEL\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml_pipeline import ESMLPipelineFactory, esml_pipeline_types\n",
    " \n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_scoring_in_folder.json',\n",
    "p.inference_mode = True\n",
    "p.active_model = 11 # 10=titanic , 11=Diabetes\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p_factory = ESMLPipelineFactory(p)\n",
    "\n",
    "# Azure machine learling pipelines need sample data to know schema\n",
    "# model_version= 0 meaning that ESML will find LATEST PROMOTED/best model, and not use a specific Model.versio to score with. It will read data from .../inference/0/... folder\n",
    "model_version = 5 # 5 = DatabricksPipeline, 36=AutoMLStep, 16=PythonPipeline, 1-4 = AtomlRun (No pipeline)\n",
    "p_factory.batch_pipeline_parameters[0].default_value = model_version \n",
    "\n",
    "training_datefolder = '1000-01-01 10:35:01.243860' # Will override active_scoring_in_folder.json'\n",
    "p_factory.batch_pipeline_parameters[1].default_value = training_datefolder # overrides ESMLProject.date_scoring_folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Legacy model: \"One time a day\" - the below is needed to be done, to ensure Azure ML v1\n",
    "- Set LEGACY mode - Azure ML v1 - since private link and DatabricksStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB! The below command you only need to run 1 time a day - then you can disable this cell. comment the code lines\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.MachineLearningServices/workspaces/aml-prj001-weu-dev-001',\n",
       " 'name': 'aml-prj001-weu-dev-001',\n",
       " 'identity': {'principal_id': '32f3a889-570a-404b-bd49-29d7b761aa13',\n",
       "  'tenant_id': '846f02b7-f92a-4053-9a99-094e5ba2e1a4',\n",
       "  'type': 'SystemAssigned'},\n",
       " 'location': 'westeurope',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {'Application Name': 'Enterprise Scale ML (ESML)',\n",
       "  'BA ID': 'NA',\n",
       "  'BCIO': 'Robin',\n",
       "  'Business Area': 'NA',\n",
       "  'Cost Center': '123456',\n",
       "  'Resource Managed By': 'The Riddler',\n",
       "  'TechnicalContact': 'batman@gothamcity.dc_',\n",
       "  'Project': 'Batcave upgrade',\n",
       "  'Description': 'ESML AI Factory'},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': '14329cb6-9c78-4714-89e1-5bde1f975f93',\n",
       " 'sdkTelemetryAppInsightsKey': '6d7b2b55-6d18-4126-a3ee-251f78cfa6e5',\n",
       " 'description': 'Azure ML workspace for prj001 in ESML-dev environment. In AI Factory(001), in westeurope',\n",
       " 'friendlyName': 'prj001-dev-001',\n",
       " 'creationTime': '2023-01-11T15:27:06.5179266Z',\n",
       " 'containerRegistry': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.ContainerRegistry/registries/acrprj001weugderqdev001',\n",
       " 'keyVault': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.Keyvault/vaults/kv-p001-weu-dev-gderq01',\n",
       " 'applicationInsights': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.insights/components/ain-prj001-weu-dev-gderq-001',\n",
       " 'storageAccount': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.Storage/storageAccounts/saprj001weugderq001dev',\n",
       " 'hbiWorkspace': False,\n",
       " 'provisioningState': 'Succeeded',\n",
       " 'imageBuildCompute': 'p001-m11weu-dev',\n",
       " 'discoveryUrl': 'https://westeurope.api.azureml.ms/discovery',\n",
       " 'privateEndpointConnections': [{'id': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.MachineLearningServices/workspaces/aml-prj001-weu-dev-001/privateEndpointConnections/aml-prj001-weu-dev-001.9c7fc1b8-4b70-4993-9c69-02de894fa09f',\n",
       "   'properties': {'provisioning_state': 'Succeeded',\n",
       "    'private_endpoint': {'id': '/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourceGroups/dc-heroes-esml-project001-weu-dev-001-rg/providers/Microsoft.Network/privateEndpoints/pend-prj001-aml-to-vnt-mlcmn'},\n",
       "    'privatelink_service_connection_state': {'status': 'Approved',\n",
       "     'description': 'Auto-Approved',\n",
       "     'actionsRequired': 'None'}}}],\n",
       " 'notebookInfo': {'fqdn': 'ml-aml-prj001-w-westeurope-14329cb6-9c78-4714-89e1-5bde1f975f93.westeurope.notebooks.azure.net',\n",
       "  'resource_id': 'b5f1067af16b4790bc3ab2dcfd6cb8d5'},\n",
       " 'v1LegacyMode': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NB! The below command you only need to run 1 time a day - then you can disable this cell. comment the code lines\")\n",
    "print(\"\")\n",
    "p.ws = p.get_workspace_from_config()\n",
    "p.ws.update(v1_legacy_mode=True) # If you happen to have a workspace in v2 mode, and want to change back to v1 legacy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `AUTO-GENERATE code: a snapshot folder`\n",
    "<a id='2_generate_snapshot_folder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT overwrite script-files with template-files such as 'scoring_gold.py', since overwrite_if_exists=False\n"
     ]
    }
   ],
   "source": [
    "## Generate CODE - then edit it to get correct environments\n",
    "p_factory.create_dataset_scripts_from_template(overwrite_if_exists=False) # Do this once, then edit them manually. overwrite_if_exists=False is DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML Workspace:\n",
      "Attached Databricks db_compute_name:\n",
      "Compute target n1-p000-aml-91 already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'step_name': 'scoring_gold',\n",
       " 'code': '/Repos/jostrm@microsoft.com/esml-aifactory002-prj002/notebook_databricks/esml/dev/project/11_diabetes_model_reg/M11/30_scoring_gold',\n",
       " 'compute_type': 'dbx',\n",
       " 'date_folder_or': None,\n",
       " 'dataset_folder_names': '',\n",
       " 'dataset_filename_ending': '*.parquet',\n",
       " 'compute_name': 'n1-p000-aml-91',\n",
       " 'cluster_id': '0111-230838-10wcl6d4'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLPipelineStepMap import IESMLPipelineStepMap\n",
    "sys.path.insert(0, \"../pipelines/M11/your_code/\")\n",
    "from ESMLPipelineStepMap import ESMLPipelineStepMap\n",
    "\n",
    "map = ESMLPipelineStepMap() # TODO 4 YOU: You need to implement this class. See \"your_code\" folder\n",
    "p_factory.use_advanced_compute_settings(map)\n",
    "\n",
    "inf_map = map.get_inference_map(p.active_model['dataset_folder_names'])\n",
    "inf_map[3] # prints  the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) `BUILDS the pipeline, and RUN the pipeline (smoke testing)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note on the `esml_pipeline_types` below, of type: esml_pipeline_types.`IN_2_GOLD_SCORING`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GEN2 as Datastore\n",
      "use_project_sp_2_mount: True\n",
      "Environment ESML-AzureML-144-AutoML_131 exists\n",
      "Using Azure ML Environment: 'ESML-AzureML-144-AutoML_131' as primary environment for PythonScript Steps\n",
      "Dataset: ds01_diabetes has advanced mapping - an Azure Databricks mapping\n",
      "Dataset: ds02_other has advanced mapping - an Azure Databricks mapping\n",
      "ESML advanced mode: with advanced compute mappings\n",
      " - Step: silver_merged_2_gold has advanced mapping - an Azure Databricks mapping\n",
      "Found attached Databricks compute cluster\n",
      "previous_step_is_databricks = 1\n",
      "create_gold_to_score_step: inference_mode=True\n",
      "par_date_utc: 1000-01-01 10:35:01.243860\n",
      "Created Databricks step in pipeline\n",
      "Adding inference step, creating...\n",
      " - Step: scoring_gold has advanced mapping - an Azure Databricks mapping for: IN_2_GOLD_SCORING_DBX\n",
      "previous_step_is_databricks = 1\n",
      "ESML INFO: SPECIFIC (maybe leading) MODEL: model_version in-parameter from user is 5, hence no guarantee that BEST LATEST PROMOTED model.version is used. User decided version is used\n",
      "ESML INFO: model_version is 5 e.g. batch_pipeline_parameters[0] = 5 meaning, it will fetch model.version=5, and READ IN DATA from .../inference/5/ folder structure\n",
      " - 1) GOLD to SCORE will be saved temporary by pipeline here: .../inference/0/gold/dev/*.parquet (e.g. overwritten each run)\n",
      " - 2) SCORED GOLD data will be saved in .../inference/5/scored/dev/run_id/*.parquet (e.g. not overwritten since run_folder, saved for each run)\n",
      " - 3) LATEST SCORED GOLD data will be saved in .../inference/0/scored/dev/run_id/*.parquet (e.g. not overwritten since run_folder. version_folder, for easy retrieval from external systems)\n",
      "par_date_utc: 1000-01-01 10:35:01.243860\n"
     ]
    }
   ],
   "source": [
    "## BUILD\n",
    "batch_pipeline = p_factory.create_batch_pipeline(esml_pipeline_types.IN_2_GOLD_SCORING_DBX) # Note the esml_pipeline_types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4a) `Execute the pipeline (smoke testing)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute_pipeline (scoring): Inference_mode: 1\n",
      "-Scoring data, default value 1000-01-01 10:35:01.243860\n",
      "Created step in2silver_ds01_diabetes [cc5c81f5][73875d3b-07f2-44ab-80b9-b9535f6f010d], (This step will run and generate new outputs)Created step in2silver_ds02_other [f5de5ff5][a583c9b8-1d32-4e38-bc7b-0622b386439a], (This step will run and generate new outputs)\n",
      "\n",
      "Created step silver_merged_2_gold [8e1ea736][19b449ea-0077-45de-8205-a41d7b86a4fe], (This step will run and generate new outputs)\n",
      "Created step scoring_gold [ee463133][6897be41-39ca-42d3-90fd-635499332715], (This step will run and generate new outputs)\n",
      "Created data reference M11_ds01_diabetes_inference_IN for StepId [ce91d52c][260a2074-efd3-41f9-8336-1554c424c306], (Consumers of this data will generate new runs.)\n",
      "Created data reference M11_ds02_other_inference_IN for StepId [7a50720c][abc46210-73cb-49ae-9419-793937440b22], (Consumers of this data will generate new runs.)\n",
      "Submitted PipelineRun e512d93a-ace7-4c91-a944-91eae6deb265\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/e512d93a-ace7-4c91-a944-91eae6deb265?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n",
      "Pipeline submitted for execution!\n",
      " ### \n",
      "PipelineRunId: e512d93a-ace7-4c91-a944-91eae6deb265\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/e512d93a-ace7-4c91-a944-91eae6deb265?wsid=/subscriptions/50ef5835-c45a-4c2e-a596-2a9e0e2a0a33/resourcegroups/dc-heroes-esml-project001-weu-DEV-001-rg/workspaces/aml-prj001-weu-DEV-001&tid=846f02b7-f92a-4053-9a99-094e5ba2e1a4\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"messageFormat\": \"Pipeline has failed child jobs. {0}\",\n        \"messageParameters\": {},\n        \"referenceCode\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2023-04-25T21:33:15.195943Z\",\n    \"componentName\": \"\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Pipeline has failed child jobs. For more details and logs, please go to the job detail page and check the child jobs.\\\",\\n        \\\"messageFormat\\\": \\\"Pipeline has failed child jobs. {0}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"referenceCode\\\": \\\"PipelineHasStepJobFailed\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2023-04-25T21:33:15.195943Z\\\",\\n    \\\"componentName\\\": \\\"\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## RUN for smoke testing purpose, to see that it works during runtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m pipeline_run \u001b[39m=\u001b[39m p_factory\u001b[39m.\u001b[39mexecute_pipeline(batch_pipeline) \u001b[39m# Tip: Pointing at the wrong folder for the sample data is the most common error \"StreamAccessException\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pipeline_run\u001b[39m.\u001b[39;49mwait_for_completion(show_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\pipeline\\core\\run.py:353\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[1;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[0;32m    351\u001b[0m error \u001b[39m=\u001b[39m final_details\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mand\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mraise\u001b[39;00m ActivityFailedException(error_details\u001b[39m=\u001b[39mjson\u001b[39m.\u001b[39mdumps(error, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m show_output:\n\u001b[0;32m    356\u001b[0m     \u001b[39mprint\u001b[39m(final_details)\n",
      "\u001b[1;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Pipeline has failed child jobs. For more details and logs, please go to the job detail page and check the child jobs.\",\n        \"messageFormat\": \"Pipeline has failed child jobs. {0}\",\n        \"messageParameters\": {},\n        \"referenceCode\": \"PipelineHasStepJobFailed\",\n        \"details\": []\n    },\n    \"environment\": \"westeurope\",\n    \"location\": \"westeurope\",\n    \"time\": \"2023-04-25T21:33:15.195943Z\",\n    \"componentName\": \"\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Pipeline has failed child jobs. For more details and logs, please go to the job detail page and check the child jobs.\\\",\\n        \\\"messageFormat\\\": \\\"Pipeline has failed child jobs. {0}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"referenceCode\\\": \\\"PipelineHasStepJobFailed\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"environment\\\": \\\"westeurope\\\",\\n    \\\"location\\\": \\\"westeurope\\\",\\n    \\\"time\\\": \\\"2023-04-25T21:33:15.195943Z\\\",\\n    \\\"componentName\\\": \\\"\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "## RUN for smoke testing purpose, to see that it works during runtime\n",
    "pipeline_run = p_factory.execute_pipeline(batch_pipeline) # Tip: Pointing at the wrong folder for the sample data is the most common error \"StreamAccessException\"\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4b) See the RESULTS: Metadata about SCORING & actual SCORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_name M11_GOLD_SCORED\n",
      "FileDataset = True\n"
     ]
    },
    {
     "ename": "UserErrorException",
     "evalue": "UserErrorException:\n\tMessage: Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: [REDACTED]\r\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6 ErrorCode: ScriptExecution.StreamAccess.NotFound\n\tInnerException \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: 853ea58d-a95b-4e7b-b3eb-1e523964d20a\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://dcheroesgderqesml001dev.dfs.core.windows.net/lake3?directory=azureml/9e7f193a-bd29-4097-ae6e-b8bcd89aef97/M11_GOLD_SCORED&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:317205f2-101f-00a6-08c7-73e54e000000\\nTime:2023-04-20T20:37:36.8869487Z\"}}\r\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\\r\\n  StreamAccessException was caused by NotFoundException.\\r\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: [REDACTED]\\r\\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6 ErrorCode: ScriptExecution.StreamAccess.NotFound\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m     meta_ds\u001b[39m.\u001b[39;49mto_pandas_dataframe()\u001b[39m.\u001b[39mhead()\n\u001b[0;32m     11\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileDataset' object has no attribute 'to_pandas_dataframe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:106\u001b[0m, in \u001b[0;36m_try_execute\u001b[1;34m(action, operation, dataset_info, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m         \u001b[39mreturn\u001b[39;00m action()\n\u001b[0;32m    107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\dataflow.py:779\u001b[0m, in \u001b[0;36mDataflow._to_pyrecords\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    778\u001b[0m         allow_fallback_to_clex \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 779\u001b[0m intermediate_files \u001b[39m=\u001b[39m _write_preppy(\u001b[39m'\u001b[39;49m\u001b[39mDataflow.to_pyrecords\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mself\u001b[39;49m, span_context\u001b[39m=\u001b[39;49mto_dprep_span_context(span\u001b[39m.\u001b[39;49mget_context()), allow_fallback_to_clex\u001b[39m=\u001b[39;49mallow_fallback_to_clex, force_clex\u001b[39m=\u001b[39;49mforce_clex)\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(intermediate_files) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:300\u001b[0m, in \u001b[0;36m_write_preppy\u001b[1;34m(activity, dataflow, force_clex, allow_fallback_to_clex, span_context, telemetry_dict)\u001b[0m\n\u001b[0;32m    299\u001b[0m     cleanup()\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (intermediate_path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_SUCCESS\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists():\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:289\u001b[0m, in \u001b[0;36m_write_preppy\u001b[1;34m(activity, dataflow, force_clex, allow_fallback_to_clex, span_context, telemetry_dict)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     _execute(\n\u001b[0;32m    290\u001b[0m         activity,\n\u001b[0;32m    291\u001b[0m         dataflow,\n\u001b[0;32m    292\u001b[0m         force_clex\u001b[39m=\u001b[39;49mforce_clex,\n\u001b[0;32m    293\u001b[0m         span_context\u001b[39m=\u001b[39;49mspan_context,\n\u001b[0;32m    294\u001b[0m         allow_fallback_to_clex\u001b[39m=\u001b[39;49mallow_fallback_to_clex,\n\u001b[0;32m    295\u001b[0m         cleanup\u001b[39m=\u001b[39;49mcleanup,\n\u001b[0;32m    296\u001b[0m         telemetry_dict\u001b[39m=\u001b[39;49mtelemetry_dict,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:608\u001b[0m, in \u001b[0;36m_execute\u001b[1;34m(activity, dataflow, is_to_pandas_dataframe, force_clex, allow_fallback_to_clex, force_preppy, collect_results, fail_on_error, fail_on_mixed_types, fail_on_out_of_range_datetime, partition_ids, traceparent, span_context, cleanup, extended_types, nulls_as_nan, telemetry_dict, convert_preppy_to_pyrecords)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m     \u001b[39mreturn\u001b[39;00m clex_execute()\n\u001b[0;32m    609\u001b[0m \u001b[39mexcept\u001b[39;00m _InconsistentSchemaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\_dataframereader.py:438\u001b[0m, in \u001b[0;36m_execute.<locals>.clex_execute\u001b[1;34m()\u001b[0m\n\u001b[0;32m    437\u001b[0m activity_data \u001b[39m=\u001b[39m Dataflow\u001b[39m.\u001b[39m_dataflow_to_anonymous_activity_data(dataflow_for_execution)\n\u001b[1;32m--> 438\u001b[0m dataflow_for_execution\u001b[39m.\u001b[39;49m_engine_api\u001b[39m.\u001b[39;49mexecute_anonymous_activity(\n\u001b[0;32m    439\u001b[0m     ExecuteAnonymousActivityMessageArguments(\n\u001b[0;32m    440\u001b[0m         anonymous_activity\u001b[39m=\u001b[39;49mactivity_data, span_context\u001b[39m=\u001b[39;49mspan_context\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m is_to_pandas_dataframe:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\_aml_helper.py:44\u001b[0m, in \u001b[0;36mupdate_aml_env_vars.<locals>.decorator.<locals>.wrapper\u001b[1;34m(op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m     43\u001b[0m     engine_api_func()\u001b[39m.\u001b[39mupdate_environment_variable(changed)\n\u001b[1;32m---> 44\u001b[0m \u001b[39mreturn\u001b[39;00m send_message_func(op_code, message, cancellation_token)\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\api.py:159\u001b[0m, in \u001b[0;36mEngineAPI.execute_anonymous_activity\u001b[1;34m(self, message_args, cancellation_token)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39m@update_aml_env_vars\u001b[39m(get_engine_api)\n\u001b[0;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_anonymous_activity\u001b[39m(\u001b[39mself\u001b[39m, message_args: typedefinitions\u001b[39m.\u001b[39mExecuteAnonymousActivityMessageArguments, cancellation_token: CancellationToken \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_message_channel\u001b[39m.\u001b[39;49msend_message(\u001b[39m'\u001b[39;49m\u001b[39mEngine.ExecuteActivity\u001b[39;49m\u001b[39m'\u001b[39;49m, message_args, cancellation_token)\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\engineapi\\engine.py:291\u001b[0m, in \u001b[0;36mMultiThreadMessageChannel.send_message\u001b[1;34m(self, op_code, message, cancellation_token)\u001b[0m\n\u001b[0;32m    290\u001b[0m     cancel_on_error()\n\u001b[1;32m--> 291\u001b[0m     raise_engine_error(response[\u001b[39m'\u001b[39;49m\u001b[39merror\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    292\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\dataprep\\api\\errorhandlers.py:10\u001b[0m, in \u001b[0;36mraise_engine_error\u001b[1;34m(error_response)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mScriptExecution\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_code:\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mraise\u001b[39;00m ExecutionError(error_response)\n\u001b[0;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mValidation\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_code:\n",
      "\u001b[1;31mExecutionError\u001b[0m: \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: 853ea58d-a95b-4e7b-b3eb-1e523964d20a\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://dcheroesgderqesml001dev.dfs.core.windows.net/lake3?directory=azureml/9e7f193a-bd29-4097-ae6e-b8bcd89aef97/M11_GOLD_SCORED&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:317205f2-101f-00a6-08c7-73e54e000000\\nTime:2023-04-20T20:37:36.8869487Z\"}}\r\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUserErrorException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mtype\u001b[39m(meta_ds) \u001b[39mis\u001b[39;00m FileDataset):\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFileDataset = True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     path \u001b[39m=\u001b[39m meta_ds\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mdownload(\u001b[39m'\u001b[39;49m\u001b[39m./data_temp/\u001b[39;49m\u001b[39m'\u001b[39;49m, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     19\u001b[0m     \u001b[39m#df = pd.DataFrame(meta_ds.to_path())\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(path)\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\_loggerfactory.py:132\u001b[0m, in \u001b[0;36mtrack.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mwith\u001b[39;00m _LoggerFactory\u001b[39m.\u001b[39mtrack_activity(logger, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[39mas\u001b[39;00m al:\n\u001b[0;32m    131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    133\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    134\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(al, \u001b[39m'\u001b[39m\u001b[39mactivity_info\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\file_dataset.py:173\u001b[0m, in \u001b[0;36mFileDataset.download\u001b[1;34m(self, target_path, overwrite, ignore_not_found)\u001b[0m\n\u001b[0;32m    165\u001b[0m dataflow \u001b[39m=\u001b[39m dataflow\u001b[39m.\u001b[39mwrite_streams(\n\u001b[0;32m    166\u001b[0m     streams_column\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPath\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    167\u001b[0m     base_path\u001b[39m=\u001b[39mbase_path,\n\u001b[0;32m    168\u001b[0m     file_names_column\u001b[39m=\u001b[39mportable_path)\n\u001b[0;32m    170\u001b[0m dataflow \u001b[39m=\u001b[39m get_dataflow_for_execution(\n\u001b[0;32m    171\u001b[0m     dataflow, \u001b[39m'\u001b[39m\u001b[39mdownload\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFileDataset\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m download_records \u001b[39m=\u001b[39m _try_execute(\n\u001b[0;32m    174\u001b[0m     dataflow\u001b[39m.\u001b[39;49m_to_pyrecords,\n\u001b[0;32m    175\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mdownload\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    176\u001b[0m     \u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mid, \u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39m'\u001b[39;49m\u001b[39mversion\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mversion})\n\u001b[0;32m    177\u001b[0m download_list \u001b[39m=\u001b[39m _get_and_validate_download_list(download_records,\n\u001b[0;32m    178\u001b[0m                                                 download_list,\n\u001b[0;32m    179\u001b[0m                                                 target_path,\n\u001b[0;32m    180\u001b[0m                                                 ignore_not_found)\n\u001b[0;32m    181\u001b[0m \u001b[39mreturn\u001b[39;00m download_list\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:108\u001b[0m, in \u001b[0;36m_try_execute\u001b[1;34m(action, operation, dataset_info, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[39mreturn\u001b[39;00m action()\n\u001b[0;32m    107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 108\u001b[0m     _handle_dataset_exception(e, dataset_info, operation)\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:98\u001b[0m, in \u001b[0;36m_handle_dataset_exception\u001b[1;34m(e, dataset_info, operation)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_handle_dataset_exception\u001b[39m(e, dataset_info\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, operation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     97\u001b[0m     message, is_dprep_exception \u001b[39m=\u001b[39m _construct_message_and_check_exception_type(e, dataset_info, operation)\n\u001b[1;32m---> 98\u001b[0m     _dataprep_error_handler(e, message, is_dprep_exception)\n",
      "File \u001b[1;32mc:\\Miniconda\\envs\\azure_automl_esml_v148\\lib\\site-packages\\azureml\\data\\dataset_error_handling.py:158\u001b[0m, in \u001b[0;36m_dataprep_error_handler\u001b[1;34m(e, message, is_dprep_exception)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m user_exception_list:\n\u001b[0;32m    157\u001b[0m         \u001b[39mif\u001b[39;00m _contains(item, \u001b[39mgetattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39merror_code\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUnexpected\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m--> 158\u001b[0m             \u001b[39mraise\u001b[39;00m UserErrorException(message, inner_exception\u001b[39m=\u001b[39me)\n\u001b[0;32m    160\u001b[0m \u001b[39mraise\u001b[39;00m AzureMLException(message, inner_exception\u001b[39m=\u001b[39me)\n",
      "\u001b[1;31mUserErrorException\u001b[0m: UserErrorException:\n\tMessage: Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: [REDACTED]\r\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6 ErrorCode: ScriptExecution.StreamAccess.NotFound\n\tInnerException \nError Code: ScriptExecution.StreamAccess.NotFound\nFailed Step: 853ea58d-a95b-4e7b-b3eb-1e523964d20a\nError Message: ScriptExecutionException was caused by StreamAccessException.\r\n  StreamAccessException was caused by NotFoundException.\r\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for 'https://dcheroesgderqesml001dev.dfs.core.windows.net/lake3?directory=azureml/9e7f193a-bd29-4097-ae6e-b8bcd89aef97/M11_GOLD_SCORED&recursive=true&resource=filesystem' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: {\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:317205f2-101f-00a6-08c7-73e54e000000\\nTime:2023-04-20T20:37:36.8869487Z\"}}\r\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed with error message: ScriptExecutionException was caused by StreamAccessException.\\r\\n  StreamAccessException was caused by NotFoundException.\\r\\n    'AdlsGen2-ListFiles (req=1, existingItems=0)' for '[REDACTED]' on storage failed with status code 'NotFound' (The specified path does not exist.), client request ID '210e8fbf-0f0b-42aa-a686-22c4e92a5294', request ID '317205f2-101f-00a6-08c7-73e54e000000'. Error message: [REDACTED]\\r\\n| session_id=699c1247-0705-4170-ab49-0f252979f0b6 ErrorCode: ScriptExecution.StreamAccess.NotFound\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data import FileDataset\n",
    "import pandas as pd\n",
    "\n",
    "ds_name =\"{}_GOLD_SCORED_RUNINFO\".format(p.ModelAlias)\n",
    "meta_ds= Dataset.get_by_name(workspace=p.ws,name=ds_name, version='latest')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "try:\n",
    "    meta_ds.to_pandas_dataframe().head()\n",
    "except AttributeError as e:\n",
    "    ds_name =\"{}_GOLD_SCORED\".format(p.ModelAlias)\n",
    "    print(\"ds_name\", ds_name)\n",
    "    meta_ds= Dataset.get_by_name(workspace=p.ws,name=ds_name, version='latest')\n",
    "\n",
    "    if(type(meta_ds) is FileDataset):\n",
    "        print(\"FileDataset = True\")\n",
    "        path = meta_ds.take(1).download('./data_temp/', overwrite=True)\n",
    "        #df = pd.DataFrame(meta_ds.to_path())\n",
    "        df = pd.DataFrame(path)\n",
    "        df.head()\n",
    "    else:\n",
    "        print(\"TabularDataset = True\")\n",
    "        print(meta_ds.to_pandas_dataframe().head())\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) PUBLISH the TRAINING pipeline & PRINT its ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLISH\n",
    "published_pipeline, endpoint = p_factory.publish_pipeline(batch_pipeline,\"_1\") # \"_1\" is optional    to create a NEW pipeline with 0 history, not ADD version to existing pipe & endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINT: Get info to use in Azure data factory\n",
    "- `published_pipeline.id` (if private Azure ML workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\n",
      "- Endpoint ID\n",
      "Endpoint ID:  aedd1e4c-a3eb-40ae-b9c2-a93b6b576523\n",
      "Endpoint Name:  11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING_EP_1\n",
      "Experiment name:  11_diabetes_model_reg_pipe_IN_2_GOLD_SCORING\n",
      "In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\n",
      "-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1930f46b-69ab-4aec-9e93-f6d5998c9e7c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"2) Fetch scored data: Below needed for Azure Data factory PIPELINE activity (Pipeline OR Endpoint. Choose the latter\") \n",
    "print (\"- Endpoint ID\")\n",
    "print(\"Endpoint ID:  {}\".format(endpoint.id))\n",
    "print(\"Endpoint Name:  {}\".format(endpoint.name))\n",
    "print(\"Experiment name:  {}\".format(p_factory.experiment_name))\n",
    "\n",
    "print(\"In AZURE DATA FACTORY - This is the ID you need, if using PRIVATE LINK, private Azure ML workspace.\")\n",
    "print(\"-You need PIPELINE id, not pipeline ENDPOINT ID ( since cannot be chosen in Azure data factory if private Azure ML)\")\n",
    "published_pipeline.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DONE! Next step would be\n",
    "\n",
    " - Q: `Next step in PRODUCTION phaase after the 2a and 3a or 3b notebooks are done?`\n",
    "\n",
    "1) Go to your ESMLProjects `Azure data factory`, and use the `ESML DataOps templates` (Azure data factory templates) for `IN_2_GOLD_SCORING`\n",
    "    - azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\adf\\v1_3\\PROJECT000\\LakeOnly\\`STEP03_IN_2_GOLD_SCORING.zip`\n",
    "2) Go to the next notebook `mlops` folder, to setup `CI/CD` in Azure Devops\n",
    "    - Import this in Azure devops\n",
    "        azure-enterprise-scale-ml\\copy_my_subfolders_to_my_grandparent\\mlops\\01_template_v14\\azure-devops-build-pipeline-to-import\\\\`ESML-v14-project002_M11-DevTest.json`\n",
    "    - Change the Azure Devops `VARIABLES` for service principle, tenant, etc.\n",
    "    - Change parameters in the `inlince Azure CLI script` to correct model you want to work with, and the correct data you want to train with, or score.\n",
    "        - File: `31-deploy_and_smoketest_batch_scoring.py`\n",
    "        - INLINE code: `--esml_model_number 11 --esml_date_utc \"1000-01-01 10:35:01.243860\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure_automl_esml_v148",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4799850f3103fb5d9644ce9433832c953591f6eac3309b593d58ecf6d126f819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
