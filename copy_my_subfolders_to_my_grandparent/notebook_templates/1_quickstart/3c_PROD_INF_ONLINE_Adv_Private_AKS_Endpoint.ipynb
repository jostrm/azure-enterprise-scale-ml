{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCTION phase: About this notebook\n",
    "- Purpose: Creates 1 AKS webservice, to serve the model as an ONLINE endpoint\n",
    "    - `AKS ONLINE Webservice:` Fetches the best trained model, Deployes that on an `AKS cluster`, always up and running, ready to be pinged and return results (via REST / Swagger, or Python SDK)\n",
    "\n",
    "## DETAILS - about this notebook and the 2 pipelines, generated            \n",
    "- 1) `Initiates ESMLProject` and sets `active model` and `active date folder`:\n",
    "- 2) `DEPLOY & SERVER: Fetched the BEST MODEL, and deploys on AKS`\n",
    "- 3) `Smoke testing: Fetches some data and calls the webservice` (smoke testing purpose - see that it works...)\n",
    "    - Gets test data\n",
    "    - Calls webservice, which both returns data via REST call, and ESML optionally also saves the returned result to datalake "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login / Switch DEV_TEST_PROD environment (1-timer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from esml import ESMLProject\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_scoring_in_folder.json',\n",
    "p.dev_test_prod=\"dev\"\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "#auth = InteractiveLoginAuthentication(force=True, tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) `Initiates ESMLProject` and sets `active model` and `active date folder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "import pandas as pd\n",
    "\n",
    "p = ESMLProject() # Will search in ROOT for your copied SETTINGS folder '../settings/model/active/active_scoring_in_folder.json',\n",
    "p.active_model = 11\n",
    "p.inference_mode = False\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\n",
    "p.verbose_logging = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) `DEPLOY`: Option A - Let ESML find BEST Model ( and its environment, scoring script) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config, model, best_run = IESMLController.get_best_model_inference_config(p.ws, p.model_folder_name, p.ModelAlias)\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_model_as_private_aks_online_endpoint(model,inference_config,overwrite_endpoint=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) `DEPLOY`: Option B - Inject YOUR selection of model and run, any model, override \"ESML best model logic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.core import Model\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "\n",
    "def get_best_model_inference_config(model_name,model_version, run_id = None):\n",
    "    print(\" - model_name {} | version {} | run_id: {}\".format(model_name,model_version,run_id))\n",
    "    model = Model(workspace=p.ws,name=model_name, version=model_version)\n",
    "    experiment = Experiment(p.ws,p.experiment_name )\n",
    "    best_run = None\n",
    "    if(run_id is not None):\n",
    "        main_run = PipelineRun(experiment=experiment, run_id=run_id)\n",
    "        best_run = main_run\n",
    "    inference_config, model, best_run = IESMLController.get_best_model_inference_config(p.ws, p.model_folder_name, p.ModelAlias,scoring_script_folder_local=None, current_model=model,run_id_tag=run_id, best_run = best_run)\n",
    "    return inference_config, model, best_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Model A: ...Deploy ANY model (not only the best promoted model)\n",
    "- Find information in Azure ML Studio in Models registry table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model 1  (Example Pipeline: DatabricksSteps + ManualML)\n",
    "model_name = 'your_model_folder_name' # '11_diabetes_model_reg' - Find a model name in Azure ML Studio/Model register or lake_settings.json\n",
    "model_version = 1 # Find a model name in Azure ML Studio/Model register\n",
    "run_id = 'todo_c70-3ef4-470c-9f55-92b33318c8ad'# '9360ac70-3ef4-470c-9f55-92b33318c8ad' # Main pipeline run - Find a model name in Azure ML Studio/Model register\n",
    "\n",
    "inference_config, model, best_run = get_best_model_inference_config(model_name,model_version,run_id)\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_model_as_private_aks_online_endpoint(model,inference_config,overwrite_endpoint=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the Run -  Example: 100% Databricks pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Model B (wrong model type): ...Deploy ANY model (not only the best promoted model)\n",
    "- `Flexibility:` You are able to, not recommended, `override the ESML validation_guard, to pass any model`:\n",
    "    ```python \n",
    "    validation_guard=False \n",
    "    ```\n",
    "\n",
    "- HOWTO TEST: If you have a model_name of a classification, and p.active_model is of ml_type regression, an error message will be triggered if validation_guard=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model 2: AutoML: Titanic classification = wrong/incompatible model. \n",
    "\n",
    "model_name = 'your_model_folder_name' # '11_diabetes_model_reg' - Find a model name in Azure ML Studio/Model register or lake_settings.json\n",
    "model_version = 1 # Find a model name in Azure ML Studio/Model register\n",
    "run_id = 'todo_c70-3ef4-470c-9f55-92b33318c8ad'# '9360ac70-3ef4-470c-9f55-92b33318c8ad' # Main pipeline run - Find a model name in Azure ML Studio/Model register\n",
    "\n",
    "inference_config, model, best_run = get_best_model_inference_config(model_name,model_version,run_id)\n",
    "\n",
    "# Problem: incorrect test_data for smoke testing will be fetched, wrong schema\n",
    "# Solution: The ESML Validation guard, will catch this, and provide solution\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_model_as_private_aks_online_endpoint(model,inference_config,overwrite_endpoint=True, validation_guard=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) `Smoke testing:` TEST ENDPOINT - Score with some test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get testdata, and score it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.connect_to_lake()\n",
    "X_test, y_test, tags = p.get_gold_validate_Xy()\n",
    "caller_id = \"10965d9c-40ca-4e47-9723-5a608a32a0e4\" # Pass an optional tracking ID for the request, parquet file will then have this name\n",
    "\n",
    "#df = p.call_webservice(p.ws, X_test,caller_id) # Saves to datalake also\n",
    "df = p.call_webservice(ws=p.ws, pandas_X_test=X_test,user_id=caller_id,firstRowOnly=True,save_2_lake_also=False) # If not saving also to datalake\n",
    "#df = p.call_webservice(ws=p.ws, pandas_X_test=X_test.iloc[:1],user_id=caller_id,firstRowOnly=True,save_2_lake_also=False) # If not saving also to datalake, predict 1 row only\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END) `SUMMARY - what did the notebook do:DEPLOY & SERVE: Fetched the BEST MODEL, and deploys on AKS`\n",
    "- ESML saves `API_key in Azure keyvault automatically`\n",
    "- ESML auto-config solves 4 common 'errors/things': `correct compute name` and `valid replicas, valid agents, valid auto scaling`\n",
    "    - Tip: You can adjust the number of replicas, and different CPU/memory configuration, or using a different compute target.\n",
    "\n",
    "### Note: AKS_SETTINGS\n",
    "- Here you can edit AKS settings (performance for DEV, TEST, PROD environments) under PROJECT specific MODEL settings and ONLINE = AKS\n",
    "    -  Link: [aks_config_dev.json](../settings/project_specific/model/dev_test_prod_override/online/aks_config_dev.json)\n",
    "- Note: \n",
    "    - Q: Why is `docker_bridge_cidr` a PUBLIC IP? Isn't this a PRIVATE AKS cluster?\n",
    "    - A: Yes, it is PRIVATE. We donâ€™t use the docker bridge for pod communication, but as Docker is configured as part of the Kubernetes setup, this docker bridge it also gets created as well, so in order to avoid that it picks random unknown CIDR that could collide with any of your existent subnets, we give the option to change it and set it a known range. So the indication for docker bridge is to define any CIDR that doesnâ€™t to Azure, and doesnâ€™t collide with any other subnet. \n",
    "        - Read more: [learn.microsoft.com](https://learn.microsoft.com/en-us/answers/questions/199786/how-to-update-docker-bridge-cidr-for-aks-to-a-diff.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `EXTRA`: Logging, edit AutoML scoring script, etc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Logging 01` - View request to webservice: Input / Output\n",
    "#### Purpose: \n",
    " - To view information logged from the score.py file, look at the traces table. The following query searches for logs where the input value was logged\n",
    " - Docs: https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-enable-app-insights#view-metrics-and-logs\n",
    " - How:\n",
    "   - 1)Open the link in Azure ML Studio, Endpoints, and property: 'Application Insights url'\n",
    "    -    Note: If this property or link is not visible, edit [aks_config_dev.json](../settings/project_specific/model/dev_test_prod_override/online/aks_config_dev.json)\n",
    "       -    `enable_app_insights:true`\n",
    "         - `collect_model_data:true`\n",
    "   - 2) Write queries from traces table - see examples below: \n",
    "\n",
    "### Application insights - examples\n",
    "- View data input to the request, see `customDimensions` in the result for this query\n",
    "\n",
    "   ```python\n",
    "   traces\n",
    "   | where customDimensions contains \"input\"\n",
    "   | limit 10\n",
    "   ```\n",
    "\n",
    "- View all events:\n",
    "   ```python\n",
    "   traces\n",
    "   | limit 10\n",
    "   ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Logging 02` - View `AMLOnlineEndpointConsoleLog`\n",
    "#### Purpose: \n",
    " - If the container fails to start, the console log may be useful for debugging.\n",
    " - performance analysis in determining the time required by the model to process each request.\n",
    " - https://learn.microsoft.com/en-us/azure/machine-learning/how-to-monitor-online-endpoints#logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print logging info - Deployment Logs \n",
    "- This will validate init() method, if model is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# load existing web service\n",
    "service = Webservice(name=\"esml-dev-p02-m11-aksapi\", workspace=p.ws)\n",
    "logs = service.get_logs()\n",
    "print(logs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESML - `save_2_lake_also`: You can also store response in datalake\n",
    "- If you are using the flag `save_2_lake_also` as below, here you will see WHERE the data is stored:\n",
    "\n",
    "    ```python\n",
    "    df = p.call_webservice(ws=p.ws, pandas_X_test=X_test.iloc[:1],user_id=caller_id,firstRowOnly=True,save_2_lake_also=True)\n",
    "    ```\n",
    "\n",
    "    The default behaviour is to store in datalake also: \n",
    "    ```python\n",
    "    df = p.call_webservice(p.ws, X_test,caller_id) # Saves to datalake also\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_score_folder, scored_folder, date_folder = p.get_gold_scored_unique_path()\n",
    "print(\"Example of where your scored data is saved. Unique folder will be different each time though\")\n",
    "print(scored_folder)\n",
    "print()\n",
    "print(\"Note: Last folder, UUID folder, should represent a 'unique scoring' for a day, but can be injected. Example: if we want a customerGUID instead \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA - HOW to customize the AutoML scoring file (that you defined earlier during the TRAIN RUN)\n",
    "- Info: If using AutoML (in pipeline as AutoMLStep or AutoMLRun), then the scoringscript file is autogenerated by Azure ML (not by ESML as it is for manual ML). \n",
    "    - AutoML will save this scoring script file at its Run in Azure ML - you then need to download it, edit it, at use the local one.\n",
    "- You need to download it locally first, then edit it, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(globals()['_dh'][0]))\n",
    "\n",
    "scoring_file = \"scoring_file_{}_automl.py\".format(p.model_folder_name)\n",
    "script_file_local = \"./settings/project_specific/model/dev_test_prod/train/ml/\"+scoring_file \n",
    "script_file_abs = os.path.abspath(script_file_local)\n",
    "\n",
    "print(\"1) Download & EDIT: Local path: to look and edit the file: {}\".format(script_file_abs))\n",
    "best_run.download_file('outputs/scoring_file_v_1_0_0.py', script_file_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2) Then set the EDITED local scoring script, to the inference_config\")\n",
    "inference_config.entry_script = script_file_abs\n",
    "\n",
    "print(\"3) Then Deploy the model\")\n",
    "service,api_uri, kv_aks_api_secret= p.deploy_model_as_private_aks_online_endpoint(model,inference_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, tags = p.get_gold_validate_Xy()\n",
    "caller_id = \"10965d9c-40ca-4e47-9723-5a608a32a0e4\" # Pass an optional tracking ID for the request, parquet file will then have this name\n",
    "\n",
    "#df = p.call_webservice(p.ws, X_test,caller_id) # Saves to datalake also\n",
    "df = p.call_webservice(ws=p.ws, pandas_X_test=X_test.iloc[:1],user_id=caller_id,firstRowOnly=True,save_2_lake_also=False) # If not saving also to datalake\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('azure_automl_esml_v144')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a3f6f829c0fbf992fdd78de6ec4e694e293d154a9b96895f90a426de0ee97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
