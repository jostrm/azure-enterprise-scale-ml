{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R&D phase: About this notebook\n",
    "- Purpose: TRAINS a model with Azure AutoML and with AZURE compute cluster and calculates test_set scoring, automatically compares if newly trained model is better.\n",
    "    - To iteratively try different ML-algorithms see what's best, change performance settings, train again.\n",
    "    - Also to try different apporoaches, classification or regression approach - which is better for the use case.\n",
    "\n",
    "- Q: `WHEN to move on form R&D phase to PRODUCTION phase notebook?`\n",
    "    - When you are happy with the MODEL (or if you have a big dataset that requires pipeline for training) - then go to the next notebook `2a_PRODUCTION_phase` to create PIPELINES: \n",
    "        - PRODUCTION PHASE & MLOps requires 1 `training pipeline`, and a `scoring pipeline` or `scoring online endpoint`, for inference \n",
    "- This notebook - Details:\n",
    "    - 1) Automaps data as Azure ML datasets. Based on your `lake_settings.json`\n",
    "    - 2) Splits the GOLD data into 3 buckets. \n",
    "        - NB this is done with local compute, not Azure, use \n",
    "             - Option 1: `2a_PRODUCTION_phase` training pipeline if data is too big for local RAM memory\n",
    "             - Option 2: Stay in this notebook & local split of data, but increase RAM memory of your/this Azure VM developer (DSVM) computer.\n",
    "             - Option 2: Stay in this notebook & local split of data, but reduce data size. Only use a sample .parquet (or .csv) file in the IN-folder.\n",
    "    - 3) Trains model\n",
    "    - 4) Registers model\n",
    "    - 5) Calculate test_set scoring\n",
    "    - 6) Deploys model - ONLINE endpoint to AKS\n",
    "    - 7) Inference: Smoke testing, using the ONLINE endpoint - get result back, saves the result to datalake also\n",
    "    - DONE.\n",
    "    \n",
    "- This notebook is called: `M10_v143_esml_classification_1_train_env_dev.ipynb` in the notebook_templates folder\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login / Switch DEV_TEST_PROD environment (1-timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from esml import ESMLProject\n",
    "\n",
    "p = ESMLProject()\n",
    "p.dev_test_prod=\"dev\"\n",
    "\n",
    "print(p.tenant)\n",
    "print(p.workspace_name) # self.workspace_name,subscription_id = self.subscription_id,resource_group = self.resource_group\n",
    "print(p.subscription_id)\n",
    "print(p.resource_group)\n",
    "\n",
    "auth = InteractiveLoginAuthentication(tenant_id = p.tenant)\n",
    "#auth = InteractiveLoginAuthentication(force=True, tenant_id = p.tenant)\n",
    "ws, config_name = p.authenticate_workspace_and_write_config(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) ESML - TRAIN Classification, TITANIC model, and DEPLOY with predict_proba scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "from esmlfac.adapter import ESMLFactory\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/esml/common/\")\n",
    "from esml import ESMLProject\n",
    "import pandas as pd\n",
    "\n",
    "param_esml_env = \"dev\" \n",
    "param_inference_model_version = \"1\" # DATALAKE(my_model/inference/active) | settings/project_specific/active/active_scoring_in_folder.json\n",
    "param_scoring_folder_date = \"1000-01-01 00:00:01.243860\" # DATALAKE(my_model/inference/active) | settings/project_specific/active/active_scoring_in_folder.json\n",
    "param_train_in_folder_date = \"1000-01-01 00:00:01.243860\" # DATALAKE(my_model/train/active) | settings/project_specific/active/active_in_folder.json\n",
    "\n",
    "p = ESMLProject(param_esml_env,param_inference_model_version,param_scoring_folder_date,param_train_in_folder_date)\n",
    "#p = ESMLProject() # Alternatively use empty contructor, which takes parameters from settings\\project_specific\\model\\active\\active_in_folder.json\n",
    "\n",
    "p.active_model = 11\n",
    "p.inference_mode = False\n",
    "p.ws = p.get_workspace_from_config() #2) Load DEV or TEST or PROD Azure ML Studio workspace\n",
    "p.verbose_logging = False\n",
    "\n",
    "# Init a ESMLController from ESMLProject configuration: Needed for \n",
    "datastore = p.connect_to_lake() # Connects to the correct ALDS GEN 2 storage account (DEV, TEST or PROD)\n",
    "controller = ESMLFactory.get_esml_controller_from_notebook(p)\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unregister_all_datasets=False\n",
    "if(unregister_all_datasets):\n",
    "    p.unregister_all_datasets(p.ws) # For DEMO purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_engieering():\n",
    "    # Feture engineering: Bronze 2 Gold - working with Azure ML Datasets with Bronze, Silver, Gold concept\n",
    "    esml_dataset = p.DatasetByName(\"ds01_diabetes\") # Get dataset\n",
    "    df_bronze = esml_dataset.Bronze.to_pandas_dataframe()\n",
    "    p.save_silver(esml_dataset,df_bronze) #Bronze -> Silver\n",
    "\n",
    "    esml_dataset2 = p.DatasetByName(\"ds02_other\") # Get dataset\n",
    "    df_bronze2 = esml_dataset2.Bronze.to_pandas_dataframe()\n",
    "    p.save_silver(esml_dataset2,df_bronze2) #Bronze -> Silver\n",
    "\n",
    "    df = esml_dataset.Silver.to_pandas_dataframe() \n",
    "    df_filtered = df[df.AGE > 0.015] \n",
    "    gold = p.save_gold(df_filtered)  #Silver -> Gold\n",
    "    return gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = None\n",
    "try:\n",
    "    datastore = p.connect_to_lake() # Connects to the correct ALDS GEN 2 storage account (DEV, TEST or PROD)\n",
    "    gold_train = p.GoldTrain\n",
    "    gold_train.name\n",
    "    print(\"Not 1st time. We have data mapped already...and splitted. Now connected to LAKE\")\n",
    "except: # If 1st time....no Gold exists, nor any mapping\n",
    "    print(\"1st time. Lets init, map what data we have in LAKE, as Azure ML Datasets\")\n",
    "    datastore = p.init() # 3) Automapping from datalake to Azure ML datasets\n",
    "    gold = test_feature_engieering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.Gold.to_pandas_dataframe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY - step 1\n",
    "- ESML has now `Automap` and `Autoregister` Azure ML Datasets as: `IN, SILVER, BRONZE, GOLD`\n",
    "- ESML has read configuration for correct environment (DEV, TEST, PROD). \n",
    "    - Both small customers, and large Enterprise customers often wants:  DEV, TEST, PROD in `diffferent Azure ML workspaces` (and different subscriptions)\n",
    "- User has done feature engineering, and saved GOLD `p.save_gold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rows in GOLD {}\".format(p.Gold.to_pandas_dataframe().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT option A) ESML default split logic, which you can override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M10_GOLD_TRAIN, M10_GOLD_VALIDATE, M10_GOLD_TEST = p.split_gold_3(0.6,label=p.active_model[\"label\"],stratified=False) # Splits and Auto-registers as AZUREM ML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT option B) Use YOUR split logic, override the default\n",
    "- You need to create your own class (ESMLSplitter is just an example class) such as MySplitter(IESMLSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../azure-enterprise-scale-ml/\")\n",
    "\n",
    "from esmlrt.interfaces.iESMLSplitter import IESMLSplitter # Just for reference to see where the abstract class exists\n",
    "from esmlrt.runtime.ESMLSplitter import ESMLSplitter1 # Point at your own code/class here instead..that needst to implement the IESMLSplitter class\n",
    "\n",
    "my_IESMLSplitter = ESMLSplitter1()\n",
    "M10_GOLD_TRAIN, M10_GOLD_VALIDATE, M10_GOLD_TEST = p.split_gold_3(train_percentage=0.6,label=p.active_model[\"label\"],stratified=False,override_with_custom_iESMLSplitter=my_IESMLSplitter) # Splits and Auto-registers as AZUREM ML Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN_2_GOLD\n",
    "- If just wanting to refine data to GOLD, for a Power BI report (No ML involved)\n",
    "- Scenario: You want to refine data from \"IN_2_GOLD\" with an easy way to READ/WRITE data (using the enterprise datalake via ESML AutoLake and ESML SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.GoldTrain.to_pandas_dataframe().head()  # Azure ML Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) `ESML` Train model in `5 codelines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We are in environment {}\".format(p.dev_test_prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our AutoML performance settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "automl_performance_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at our label, and our machine learning task type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Label is: {}'.format(p.active_model[\"label\"]))\n",
    "print('ml_type / task is: {}'.format(p.active_model[\"ml_type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets TRAIN with AutoML & Azure compute cluster (M11 demo takes ~ 10-15min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esml import ESMLProject\n",
    "from baselayer_azure_ml import AutoMLFactory,azure_metric_regression,azure_metric_classification\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_performance_config = p.get_automl_performance_config() # 1)Get config, for active environment (dev,test or prod)\n",
    "aml_compute = p.get_training_aml_compute(p.ws) # 2)Get compute, for active environment\n",
    "\n",
    "automl_config = AutoMLConfig(task = p.active_model[\"ml_type\"], # 4) Override the ENV config, for model(that inhertits from enterprise DEV_TEST_PROD config baseline)\n",
    "                            primary_metric = p.active_model[\"ml_metric\"], #  Note: Regression[MAE, RMSE,R2,Spearman] Classification[AUC,Accuracy,Precision,Precision_avg,Recall]\n",
    "                            compute_target = aml_compute,\n",
    "                            training_data = p.GoldTrain, # is 'train_6' pandas dataframe, but as an Azure ML Dataset\n",
    "                            experiment_exit_score = p.active_model[\"ml_time_out_score\"], # DEMO purpose. remove experiment_exit_score if you want to have good accuracy (put a comment # on this row to remove it)\n",
    "                            label_column_name = p.active_model[\"label\"],\n",
    "                            **automl_performance_config\n",
    "                        )\n",
    "\n",
    "best_run, fitted_model, experiment = AutoMLFactory(p).train_as_run(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) ESML Scoring Drift/Concept Drift: Compare with `1-codeline`: Promote model or not? If better, then `Register model`\n",
    "- `IF` newly trained model in `current` environment (`DEV`, `TEST` or `PROD`) scores BETTER than existing model in `target` environment, then `new model` can be registered and promoted.\n",
    "- Q: Do we have `SCORING DRIFT / CONCEPT DRIFT?`\n",
    "- Q: Is a model trained on NEW data better? IS the one in production degraded? (not fit for the data it scores - real world changed, other CONCEPT)\n",
    "- A: - Lets check. Instead of `DataDrift`, lets look at `actual SCORING` on new data (and/or new code, feature engineering) - See if we should PROMOTE newly trained model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current AI Factory environment: '{}' - AML WS: '{}'\".format(p.dev_test_prod, p.ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if we already have a MODEL with a suitable NAME - to gruoup ur runs and model versions under.\n",
    "- Purpose: Gets consitent model name, if many runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "current_model,run_id_tag, model_name = IESMLController.get_best_model_via_modeltags_only_DevTestProd(p.ws,controller.experiment_name)\n",
    "\n",
    "if(current_model is None):\n",
    "    print(\"No existing model with experiment name {}. The Model name will now be same as experiment name\".format(controller.experiment_name))\n",
    "    current_model = None\n",
    "    run_id_tag = \"\"\n",
    "    model_name = controller.experiment_name\n",
    "else:\n",
    "    print(\"Current BEST model is: {} from Model registry with experiment_name-TAG {}, run_id-TAG {}  model_name-TAG {}\".format(current_model.name,controller.experiment_name,run_id_tag,model_name))\n",
    "    if (\"esml_time_updated\" in current_model.tags):\n",
    "        print(\"esml_time_updated: {}\".format(current_model.tags.get(\"esml_time_updated\")))\n",
    "    print(\"status_code : {}\".format(current_model.tags.get(\"status_code\")))\n",
    "    print(\"model_name  : {}\".format(current_model.tags.get(\"model_name\")))\n",
    "    print(\"trained_in_workspace   : {}\".format(current_model.tags.get(\"trained_in_workspace\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register new trained model, as NEW: not promoted.\n",
    " - Purpose: To be able to TAG scoring on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "import datetime\n",
    "\n",
    "time_stamp = str(datetime.datetime.now())\n",
    "ml_flow_stage = IESMLController._get_flow_equivalent(IESMLController.esml_status_new)\n",
    "\n",
    "tags = {\"esml_time_updated\": time_stamp,\"status_code\": IESMLController.esml_status_new,\"mflow_stage\":ml_flow_stage, \"run_id\": best_run.id, \"model_name\": model_name, \"trained_in_environment\": controller.dev_test_prod, \n",
    "    \"trained_in_workspace\": p.ws.name, \"experiment_name\": controller.experiment_name, \"trained_with\": \"AutoMLRun\"}\n",
    "\n",
    "model = best_run.register_model(model_name=model_name, tags=tags, description=\"\", model_path=\".\")\n",
    "print(\"model.name\", model.name)\n",
    "print(\"model.version\", model.version)\n",
    "#model_path = None\n",
    "#model = controller._register_aml_model(model_path,model_name,tags,ws,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SET SCORING: Calculate test_set SCORING\n",
    "- Is tagged on MODEL in Azure ML Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rehydrate RUN - to calulate test_scoring\n",
    "- if you restarted notebook, and dont want to wait for TRAIN again, you can fetch RUN, FITTED_MODEL, AML_MODEL as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "\n",
    "'''\n",
    "your_model_id = \"AutoMLd123123\" # See Azure ML Studio - Models registry, 1st column in table\n",
    "models_run_id = \"AutoML_asdf123\" # See Azure ML Studio - Models registry, 2nd column in table. If empty, see JOBS id for run_id\n",
    "model = Model(p.ws, your_model_id)\n",
    "run,best_run,fitted_model = IESMLController.init_run(p.ws,controller.experiment_name, models_run_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, rmse, r2, mean_abs_percent_error,mae,spearman_correlation,plt, dummy = controller.ESMLTestScoringFactory.get_test_scoring_8(\n",
    "    p.ws,\n",
    "    p.active_model[\"label\"],\n",
    "    p.GoldTest,\n",
    "    fitted_model,\n",
    "    best_run,\n",
    "    model)\n",
    "\n",
    "print(\"Scoring for NEW model is: {},{},{},{},{}\".format(rmse,r2,mean_abs_percent_error,mae,spearman_correlation))\n",
    "\n",
    "a_scoring = \"\"\n",
    "if (controller.ESMLTestScoringFactory.ml_type == \"regression\"):\n",
    "    a_scoring = model.tags.get(\"test_set_R2\")\n",
    "    print(\"RMSE:\")\n",
    "    print(rmse)\n",
    "    print()\n",
    "    print(\"R2:\")\n",
    "    print(r2)\n",
    "    print()\n",
    "    print(\"MAPE:\")\n",
    "    print(mean_abs_percent_error)\n",
    "    print()\n",
    "    print(\"MAE:\")\n",
    "    print(mae)\n",
    "    print()\n",
    "    print(\"Spearman:\")\n",
    "    print(spearman_correlation)\n",
    "elif (controller.ESMLTestScoringFactory.ml_type == \"classification\"):\n",
    "    a_scoring = model.tags.get(\"test_set_Accuracy\")\n",
    "print(\"Verifying that at least 1 scoring exists in TAGS on model: {}\".format(a_scoring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare - INNER LOOP & Register with PROMOTED status, if better\n",
    " - Better than other in DEV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROMOTE model - INNER LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(best_run is not None):\n",
    "    print(best_run.parent.id)\n",
    "if(run is not None):    \n",
    "    print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esmlrt.interfaces.iESMLController import IESMLController\n",
    "\n",
    "dev_ws = p.ws\n",
    "esml_current_env = \"dev\"\n",
    "next_environment=\"dev\"\n",
    "#target_ws = controller.get_target_workspace(current_environment = esml_current_env, current_ws = dev_ws, target_environment = esml_current_env)\n",
    "target_ws = dev_ws\n",
    "\n",
    "if(run is None):\n",
    "    run_id = best_run.parent.id # This is set if you just ran the TRAIN cell in this notebook. AutoMLRun in notebook - we need its parent.\n",
    "else:\n",
    "    run_id = run.id # Rehydrated run=parent which is set in a CELL above in this noteboo you may use. If not having a fresh training in RAM.\n",
    "\n",
    "run_id = IESMLController.get_safe_automl_parent_run_id(run_id)\n",
    "promote_new_model,source_model_name,source_run_id,source_best_run,source_model,leading_model = controller.ESMLComparer.compare_scoring_current_vs_new_model(\n",
    "    new_run_id = run_id,#run_id_tag, #automl_step_run_id,\n",
    "    new_model = None,\n",
    "    model_name = model.name,\n",
    "    current_ws = dev_ws,\n",
    "    current_environment = esml_current_env,\n",
    "    target_environment = next_environment,\n",
    "    target_workspace = target_ws,\n",
    "    experiment_name = controller.experiment_name)\n",
    "\n",
    "if(source_best_run.id == run_id):\n",
    "    print(\"Correct RUN found. Parent run.\")\n",
    "\n",
    "print(\"INNER LOOP (dev->dev) - PROMOTE?\")\n",
    "if (promote_new_model == True): # Better than all in DEV/Curren environment?\n",
    "    model_registered_in_target = controller.register_model(source_ws=p.ws, target_env=esml_current_env, source_model=model, run=source_best_run,esml_status=IESMLController.esml_status_promoted_2_dev) \n",
    "    print(\"Promoted model! in environment {}\".format(esml_current_env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG cells (in mardkdown \"M\", selected and press \"Y\" to get CODE cells) - before `PROMOTE model - INNER LOOP`\n",
    "- Purpose: Rehydrate Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core import Experiment\n",
    "id_1 = best_run.parent.id\n",
    "\n",
    "print(controller.experiment_name)\n",
    "print(id_1)\n",
    "exp = Experiment(p.ws,controller.experiment_name)\n",
    "\n",
    "run = AutoMLRun(experiment=exp, run_id=id_1)\n",
    "best_run, fitted_model = run.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG cell - after a TRAIN run\n",
    "- The train run, will generate a `temporary model_name (Azure ML will not update, when renamed at registration)`. The tag that Azure ML writes: `best_run.properties['model_name']`,this is NOT the correct model_name (since not same after REGISTRATION)\n",
    "- Why having your OWN model name? Since we in ESML want to \"lookup\" if a model name already exists, under same MODEL NAME - to collect all under same model name, but with versions. \n",
    "    - Aml may create a new random name under same experiment after a couple of runs. Hence good to have your own \"known\". Example: Stick with the 1st generated name AML creates for you.\n",
    "    - ESML also collects all models under same `experiment name TAG`, since you can TRAIN a model from a NOTEBOOK, or from a PIPELINE, and these will have different EXPERIMENT NAMES, hence using a TAG with a common name\n",
    "    - All and all for MLOps: This to be able to determing BEST promoted model, and LATEST challenger model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "my_model_name_that_exists_in_registry = \"AutoMLd123123\" # Look in Model registry for a model name that exists\n",
    "\n",
    "print(best_run.id)\n",
    "model_name1 = best_run.properties['model_name']\n",
    "print(model_name1)\n",
    "\n",
    "try:\n",
    "    model_again = Model(p.ws, model_name1) # This will probably not be found, since this is not the registered models name, it only was the name temporary in the RUN..and is NOT updated when renamed.\n",
    "    print(model_again.name)\n",
    "except:\n",
    "    print(\"could not find a registered model with name {}. This is due we CUSTOMIZE the name, when register it. This is not updated on run.properties.\".format(model_name1))\n",
    "    model_again = Model(p.ws, my_model_name_that_exists_in_registry) \n",
    "    print(model_again.name)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG cell - when SCORING pipeline runs, you pass MODEL VERSION, if VERSION=0...\n",
    "...then LATEST PROMOTED model is used to score with\n",
    "\n",
    "Below you can see how to HYDRATE the fitted_model, BEST_RUN and BEST_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_version_in_int = 0\n",
    "print(\"Fetching BEST MODEL that is promoted. To get its name\")\n",
    "current_model2,run_id_tag, model_name = IESMLController.get_best_model_via_modeltags_only_DevTestProd(p.ws,p.model_folder_name)\n",
    "\n",
    "if(current_model2 is None):\n",
    "    print(\"No existing model with experiment name {}. The Model name will now be same as experiment name\".format(p.model_folder_name))\n",
    "if(model_version_in_int == 0):\n",
    "    print(\"Initiating BEST MODEL - PROMOTED leading model (since model_version=0). Hydrating to get its run and fitted model.\")\n",
    "\n",
    "    run_id_2 = current_model2.tags.get(\"run_id\")\n",
    "    safe_run_id = IESMLController.get_safe_automl_parent_run_id(run_id_2)\n",
    "    run2,best_run2,fitted_model2 = IESMLController.init_run(p.ws,p.model_folder_name, safe_run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('azure_automl_esml_v144')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a3f6f829c0fbf992fdd78de6ec4e694e293d154a9b96895f90a426de0ee97e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
